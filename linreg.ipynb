{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with one variable\n",
    "\n",
    "Consider the data set $(x^{(1)}, y^{(1)}), \\dots, (x^{(m)}, y^{(m)})$, where $x$ is the independent variable and $y$ is the dependent variable. We will model this data set as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\alpha + \\beta x.\n",
    "\\end{equation}\n",
    "\n",
    "What are the values of $\\alpha$ and $\\beta$ that provide the best fit to the data set? To answer this question, we minimize the sum of squared errors:\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\alpha, \\beta) &= \\frac{1}{2m}\\sum_{i=1}^N (f(x^{(i)}) - y^{(i)})^2 \\\\\n",
    "                 &= \\frac{1}{2m}\\sum_{i=1}^N (\\alpha + \\beta x^{(i)} - y^{(i)})^2.\n",
    "\\end{align*}\n",
    "\n",
    "$J(\\alpha, \\beta)$ is called the *cost function*, or *objective function*. Let's minimize the cost function with respect to $\\alpha$ and $\\beta$. We will denote the optimal values by $\\hat{\\beta}$ and $\\hat{\\alpha}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial\\alpha} = 0 &\\to \\frac{1}{m}\\sum_{i=1}^m (\\hat{\\alpha} + \\hat{\\beta}x^{(i)} - y^{(i)}) = 0 \\\\\n",
    "\\frac{\\partial J}{\\partial\\beta}  = 0 &\\to \\frac{1}{m}\\sum_{i=1}^m (\\hat{\\alpha} + \\hat{\\beta}x^{(i)} - y^{(i)})x^{(i)} = 0.\n",
    "\\end{align*}\n",
    "\n",
    "Consider the following definitions:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{x} = \\frac{1}{m}\\sum_{i=1}^m x^{(i)} ,\\qquad\n",
    "\\bar{y} = \\frac{1}{m}\\sum_{i=1}^m y^{(i)} ,\\qquad\n",
    "S_x^2   = \\frac{1}{m}\\sum_{i=1}^m (x^{(i)} - \\bar{x})^2 ,\\qquad\n",
    "C_{xy}  = \\frac{1}{m}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y}).\n",
    "\\end{equation}\n",
    "\n",
    "These quantities should be familiar: $\\bar{x}$ is the sample mean of $x$, $\\bar{y}$ is the sample mean of $y$, $S_x^2$ is the (biased) sample variance of $x$, and $C_{xy}$ is the (biased) sample covariance of $x$ and $y$. Using these definitions, it can be shown after straightforward algebra that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} = \\frac{C_{xy}}{S_x^2}, \\qquad\n",
    "\\hat{\\alpha} = \\bar{y} - \\hat{\\beta}\\bar{x}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with `statsmodels`\n",
    "\n",
    "Below we will create a fake dataset, run linear regression on it using `statsmodels`, and try to reproduce the various outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels import regression\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data using the model $f(x)=0.3 + 2x$ and Gaussian noise with $\\sigma=0.5$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11254b290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make results reproducible\n",
    "np.random.seed(123)\n",
    "\n",
    "m = 100\n",
    "x = np.linspace(0, 1, m)\n",
    "y = np.random.normal(loc=2.0*x+0.3, scale=0.5, size=m)\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run regression using `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.515</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   106.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 09 Mar 2019</td> <th>  Prob (F-statistic):</th> <td>2.63e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:42:42</td>     <th>  Log-Likelihood:    </th> <td> -84.642</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   173.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   178.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.3065</td> <td>    0.113</td> <td>    2.710</td> <td> 0.008</td> <td>    0.082</td> <td>    0.531</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    2.0140</td> <td>    0.195</td> <td>   10.306</td> <td> 0.000</td> <td>    1.626</td> <td>    2.402</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.753</td> <th>  Durbin-Watson:     </th> <td>   1.975</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.252</td> <th>  Jarque-Bera (JB):  </th> <td>   1.746</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.035</td> <th>  Prob(JB):          </th> <td>   0.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.356</td> <th>  Cond. No.          </th> <td>    4.35</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.520\n",
       "Model:                            OLS   Adj. R-squared:                  0.515\n",
       "Method:                 Least Squares   F-statistic:                     106.2\n",
       "Date:                Sat, 09 Mar 2019   Prob (F-statistic):           2.63e-17\n",
       "Time:                        16:42:42   Log-Likelihood:                -84.642\n",
       "No. Observations:                 100   AIC:                             173.3\n",
       "Df Residuals:                      98   BIC:                             178.5\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.3065      0.113      2.710      0.008       0.082       0.531\n",
       "x1             2.0140      0.195     10.306      0.000       1.626       2.402\n",
       "==============================================================================\n",
       "Omnibus:                        2.753   Durbin-Watson:                   1.975\n",
       "Prob(Omnibus):                  0.252   Jarque-Bera (JB):                1.746\n",
       "Skew:                           0.035   Prob(JB):                        0.418\n",
       "Kurtosis:                       2.356   Cond. No.                         4.35\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = regression.linear_model.OLS(y, np.column_stack([np.ones(len(x)), x])).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal parameters\n",
    "\n",
    "Let's see if the formulas we derived for $\\hat{\\beta}$ and $\\hat{\\alpha}$ match the output of `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Alpha = 0.306537621742\n",
      "Reproduced  Alpha = 0.306537621742\n",
      "\n",
      "statsmodels Beta  = 2.01403383001\n",
      "Reproduced  Beta  = 2.01403383001\n"
     ]
    }
   ],
   "source": [
    "xmean = np.mean(x)\n",
    "xvar = np.var(x)\n",
    "ymean = np.mean(y)\n",
    "\n",
    "beta = np.cov(x, y, bias=True)[0, 1] / xvar\n",
    "alpha = ymean - beta * xmean\n",
    "print(\"statsmodels Alpha = {}\".format(model.params[0]))\n",
    "print(\"Reproduced  Alpha = {}\".format(alpha))\n",
    "print(\"\")\n",
    "print(\"statsmodels Beta  = {}\".format(model.params[1]))\n",
    "print(\"Reproduced  Beta  = {}\".format(beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the best estimates $\\hat{\\beta}$ and $\\hat{\\alpha}$ are quite close to the actual model parameters $\\beta=2$ and $\\alpha=0.3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-squared\n",
    "\n",
    "R-squared (usually denoted $R^2$) is defined as \n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^m(y^{(i)} - f^{(i)})^2}{\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2},\n",
    "\\end{equation}\n",
    "\n",
    "where $f(x)$ is any model of the data and $f^{(i)} = f(x^{(i)})$. R-squared compares the error in prediction (or *residual*) $y^{(i)} - f^{(i)}$ to that of the trivial model $f=\\bar{y}$. The smaller the residual, the closer $R^2$ is to $1$. In that sense, it measures the proportion of variance in the dependent variable $y$ that is predictable from the independent variable $x$ using the model $f$. Note, however, that this is an *in-sample* measure of goodness-of-fit, if the same dataset is used to fit the parameters of the model *and* compute $R^2$.\n",
    "\n",
    "Let's confirm $R^2$ for our linear regression model above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels R-squared = 0.520089566727\n",
      "Reproduced  R-squared = 0.520089566727\n"
     ]
    }
   ],
   "source": [
    "f = alpha + beta * x\n",
    "e = y - f\n",
    "rsquared = 1 - np.dot(e, e) / np.dot(y - ymean, y - ymean)\n",
    "print(\"statsmodels R-squared = {}\".format(model.rsquared))\n",
    "print(\"Reproduced  R-squared = {}\".format(rsquared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model $f$ is linear regression with an intercept term, $R^2$ is equal to the square of the sample correlation between $f$ and $y$. Let's check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5200895667266312"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(f, y)[0, 1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed! Let's prove this.\n",
    "\n",
    "Consider the following definitions:\n",
    "\n",
    "\\begin{equation}\n",
    "S_y^2   = \\frac{1}{m}\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2 ,\\qquad\n",
    "r_{xy}  = \\frac{C_{xy}}{S_xS_y},\n",
    "\\end{equation}\n",
    "\n",
    "where $S_y$ is the (biased) sample variance of $y$ and $r_{xy}$ is the sample correlation coefficient of $x$ and $y$. Using this, we can express $\\hat{\\beta}$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} = \\frac{r_{xy}S_y}{S_x}.\n",
    "\\end{equation}\n",
    "\n",
    "Also note that:\n",
    "\n",
    "\\begin{equation}\n",
    "f^{(i)} - \\bar{y} = \\hat{\\alpha} + \\hat{\\beta}x^{(i)} - \\bar{y} = \\hat{\\beta}(x^{(i)}-\\bar{x}),\n",
    "\\end{equation}\n",
    "\n",
    "where the second equality follows from $\\hat{\\alpha}=\\bar{y} - \\hat{\\beta}\\bar{x}$. The average of squared residuals can be written as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{1}{m}\\sum_{i=1}^m(f^{(i)} - y^{(i)})^2\n",
    "&= \\frac{1}{m}\\sum_{i=1}^m[(f^{(i)} - \\bar{y}) - (y^{(i)} - \\bar{y}) ]^2 \\\\\n",
    "&= \\frac{1}{m}\\sum_{i=1}^m(f^{(i)} - \\bar{y})^2 + \\frac{1}{m}\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2 - \\frac{2}{m}\\sum_{i=1}^m(f^{(i)} - \\bar{y})(y^{(i)} - \\bar{y}) \\\\\n",
    "&= \\frac{\\hat{\\beta}^2}{m}\\sum_{i=1}^m(x^{(i)} - \\bar{x})^2 + S_y^2 - \\frac{\\hat{\\beta}}{m}\\sum_{i=1}^m(x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y}) \\\\\n",
    "&= \\hat{\\beta}^2 S_x^2 + S_y^2 - 2\\hat{\\beta}C_{xy} \\\\\n",
    "&= S_y^2(1 - r_{xy}^2),\n",
    "\\end{align*}\n",
    "\n",
    "where the third equality follows from $f^{(i)} - \\bar{y} = \\hat{\\beta}(x^{(i)}-\\bar{x})$, and the last one from substituting the value of $\\hat{\\beta}$.\n",
    "\n",
    "Finally:\n",
    "\n",
    "\\begin{align*}\n",
    "R^2 &= 1 - \\frac{\\frac{1}{m}\\sum_{i=1}^m(y^{(i)} - f^{(i)})^2}{\\frac{1}{m}\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2} \\\\\n",
    "    &= 1 - \\frac{S_y^2(1 - r_{xy}^2)}{S_y^2} \\\\\n",
    "    &= r_{xy}^2.\n",
    "\\end{align*}\n",
    "\n",
    "We proved that $R^2$ is given by the square of the sample correlation between $x$ and $y$, and not $f$ and $y$ as originally promised! First, let's numerically check what we just proved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5200895667266315"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(x, y)[0, 1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Now let's do the extra work and show that $r_{fy}^2 = r_{xy}^2$.\n",
    "\n",
    "To do this, we'll need one more result:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{f}\n",
    "    = \\frac{1}{m}\\sum_{i=1}^m f^{(i)}\n",
    "    = \\frac{1}{m}\\sum_{i=1}^m (\\hat{\\alpha} + \\hat{\\beta}x^{(i)})\n",
    "    = \\hat{\\alpha} + \\hat{\\beta}\\bar{x}\n",
    "    = \\bar{y},\n",
    "\\end{equation}\n",
    "where the last line follows from $\\hat{\\alpha} = \\bar{y} - \\hat{\\beta}\\bar{x}$.\n",
    "\n",
    "Now we're ready:\n",
    "\n",
    "\\begin{align*}\n",
    "r_{fy}\n",
    "&= \\frac{\\sum_{i=1}^m (f^{(i)} - \\bar{f})(y^{(i)} - \\bar{y})}{\\sqrt{\\sum_{i=1}^m (f^{(i)} - \\bar{f})^2}\\sqrt{\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2}} \\\\\n",
    "&= \\frac{\\sum_{i=1}^m (f^{(i)} - \\bar{y})(y^{(i)} - \\bar{y})}{\\sqrt{\\sum_{i=1}^m (f^{(i)} - \\bar{y})^2}\\sqrt{\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2}} \\\\\n",
    "&= \\frac{\\hat{\\beta}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y})}{\\sqrt{\\hat{\\beta}^2\\sum_{i=1}^m (x^{(i)} - \\bar{x})^2}\\sqrt{\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2}} \\\\\n",
    "&= \\frac{\\hat{\\beta}}{|\\hat{\\beta}|}r_{xy},\n",
    "\\end{align*}\n",
    "\n",
    "where the second equality follows from $\\bar{f}=\\bar{y}$ and the third equality follows from $f^{(i)} - \\bar{y} = \\hat{\\beta}(x^{(i)}-\\bar{x})$. There we have it:\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = r_{fy}^2 = r_{xy}^2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic view\n",
    "\n",
    "In most realistic cases, we cannot hope to explain all variability in $y$ using only $x$. This is also true for models which are more complex than linear regression. We should always expect a certain degree of randomness that our models cannot account for. However, we can hope to design models that make the correct predictions *on average*. This suggests a probabilistic approach for modelling. We can think about $y$ as a random variable with a certain probability distribution, whose mean is given by the model $f(x)$.\n",
    "\n",
    "For simplicity, let's assume $y^{(i)}$ are independent normally-distributed samples with standard deviation $\\sigma$: \n",
    "\n",
    "\\begin{equation}\n",
    "y^{(i)} \\sim \\mathcal{N}(\\alpha + \\beta x^{(i)}, \\sigma^2)\n",
    "\\end{equation}\n",
    "\n",
    "The probabiliy of observing $(x^{(i)}, y^{(i)})$ is\n",
    "\n",
    "\\begin{equation}\n",
    "P^{(i)} = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp{\\left[-\\frac{(y^{(i)} - \\alpha - \\beta x^{(i)})^2}{2\\sigma^2}\\right]}.\n",
    "\\end{equation}\n",
    "\n",
    "Since we're assuming all observations are independent, the probability of observing the whole dataset $(x^{(1)}, y^{(1)}), \\dots, (x^{(m)}, y^{(m)})$ is\n",
    "\n",
    "\\begin{equation}\n",
    "P = \\Pi_{i=1}^m P^{(i)} = \\frac{1}{(2\\pi \\sigma^2)^{m/2}}\\exp{\\left[-\\sum_{i=1}^m\\frac{(y^{(i)} - \\alpha - \\beta x^{(i)})^2}{2\\sigma^2}\\right]} = \\frac{1}{(2\\pi \\sigma^2)^{m/2}}\\exp{\\left(-\\frac{mJ(\\alpha, \\beta)}{\\sigma^2}\\right)},\n",
    "\\end{equation}\n",
    "\n",
    "where $J$ is the cost function defined earlier. This is interesting! The probability of observing the data set can be expressed in terms of the cost function. In fact, the parameters $\\alpha$ and $\\beta$ that minimize $J$ also maximize $P$. Picking the parameters of a model to maximize the probability of observing the dataset is called *maximum likelihood estimation*.\n",
    "\n",
    "*Log-likelihood* is given by $\\log P$, which in our case is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\log P(\\alpha, \\beta, \\sigma^2) = -\\frac{m}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^m(y^{(i)} - \\alpha - \\beta x^{(i)})^2.\n",
    "\\end{equation}\n",
    "\n",
    "In the model summary above from `statsmodels`, a number is quoted for Log-Likelihood. To reproduce it, let's compute $\\log P(\\hat{\\alpha}, \\hat{\\beta}, \\sigma^2)$. One thing, though, is that $\\log P$ depends on $\\sigma$, which `statsmodels` doesn't take as input. It can, however, esimate it from the sample standard deviation of the prediction errors $y^{(1)} - f^{(1)}, \\dots, y^{(m)} - f^{(m)}$. Let's give that a go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Log-Likelihood = -84.6424357588\n",
      "Reproduced  Log-Likelihood = -84.6424357588\n"
     ]
    }
   ],
   "source": [
    "loglikelihood = -(len(x)/2.0)*np.log(2*np.pi*np.var(e)) - m/2.0\n",
    "print(\"statsmodels Log-Likelihood = {}\".format(model.llf))\n",
    "print(\"Reproduced  Log-Likelihood = {}\".format(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error of estimates\n",
    "\n",
    "Given the probabilistic view, we now see that the optimal estimates $\\hat{\\alpha}$ and $\\hat{\\beta}$ should be regarded as random variables, and not confused with the \"real\" underlying parameters $\\alpha$ and $\\beta$. A different dataset, for instance, would result in different values for $\\hat{\\alpha}$ and $\\hat{\\beta}$. Therefore, it's important to study their variability.\n",
    "\n",
    "Let's first check their means. Note that by assumption\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}[y^{(i)}] = \\alpha + \\beta x^{(i)},\n",
    "\\end{equation}\n",
    "\n",
    "and as a result\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[\\bar{y}]\n",
    "    &= \\frac{1}{m} \\sum_{i=1}^m \\mathbb{E}[y^{(i)}] \\\\\n",
    "    &= \\frac{1}{m} \\sum_{i=1}^m (\\alpha + \\beta x^{(i)}) \\\\\n",
    "    &= \\alpha + \\beta \\bar{x}.\n",
    "\\end{align*}\n",
    "\n",
    "Combining these:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}[y^{(i)} - \\bar{y}] = \\beta (x^{(i)} - \\bar{x}).\n",
    "\\end{equation}\n",
    "\n",
    "Given these results, we can compute the mean of $\\hat{\\beta}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[\\hat{\\beta}]\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})\\mathbb{E}[y^{(i)} - \\bar{y}] \\\\\n",
    "    &= \\frac{\\beta}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(x^{(i)} - \\bar{x}) \\\\\n",
    "    &= \\beta,\n",
    "\\end{align*}\n",
    "\n",
    "and also that of $\\hat{\\alpha}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[\\hat{\\alpha}]\n",
    "    &= \\mathbb{E}[\\bar{y}] - \\mathbb{E}[\\hat{\\beta}] \\bar{x} \\\\\n",
    "    &= \\alpha + \\beta \\bar{x} - \\beta \\bar{x} \\\\\n",
    "    &= \\alpha.\n",
    "\\end{align*}\n",
    "\n",
    "So the mean of $\\hat{\\beta}$ and $\\hat{\\alpha}$ are $\\beta$ and $\\alpha$, respectively. Good, but no terribly surprising!\n",
    "\n",
    "Let's compute the variance of $\\hat{\\beta}$. First, we will express $\\hat{\\beta} - \\beta$ in a convenient way:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\beta} - \\beta\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y}) - \\frac{\\beta}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})^2 \\\\\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})[(y^{(i)} - \\beta x^{(i)} - \\alpha) - (\\bar{y} - \\beta \\bar{x} - \\alpha)] \\\\\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\mathbb{E}[y^{(i)}]) - \\frac{\\bar{y} - \\beta \\bar{x} - \\alpha}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x}) \\\\\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\mathbb{E}[y^{(i)}])\n",
    "\\end{align*}\n",
    "\n",
    "where the first equality follows from the definition of $S_x^2$, the second from $\\mathbb{E}[y^{(i)}] = \\alpha + \\beta x^{(i)}$, and the third from $\\sum_{i=1}^m (x^{(i)} - \\bar{x})=0$. Then:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Var}[\\hat{\\beta}]\n",
    "    &= \\mathbb{E}[(\\hat{\\beta} - \\beta)^2] \\\\\n",
    "    &= \\frac{1}{m^2S_x^4}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x})\\mathbb{E}[(y^{(i)} - \\mathbb{E}[y^{(i)}]) (y^{(j)} - \\mathbb{E}[y^{(j)}])] \\\\\n",
    "    &= \\frac{1}{m^2S_x^4}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x})\\text{Cov}[y^{(i)}, y^{(j)}] \\\\\n",
    "    &= \\frac{1}{m^2S_x^4}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x})\\sigma^2\\delta_{ij} \\\\\n",
    "    &= \\frac{\\sigma^2}{m^2S_x^4}\\sum_{i}^m (x^{(i)} - \\bar{x})^2 \\\\\n",
    "    &= \\frac{\\sigma^2}{mS_x^2},\n",
    "\\end{align*}\n",
    "\n",
    "where the second equality follows from the fact that $y^{(1)}, \\dots, y^{(m)}$ are (by assumption) independent and normally distributed with variance $\\sigma^2$, i.e $\\text{Cov}[y^{(i)}, y^{(j)}] = \\sigma^2\\delta_{ij}$.\n",
    "\n",
    "Let's check this against `statsmodels`. The estimated value of $\\sigma^2$ (since the real value is unknown to `statsmodels`), is stored in the `scale` attribute of `model`. Using that, we can reproduce the standard error of $\\hat{\\beta}$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Beta std err = 0.195431582311\n",
      "Reproduced  Beta std err = 0.195431582311\n"
     ]
    }
   ],
   "source": [
    "beta_se = np.sqrt(model.scale / (m * xvar))\n",
    "print(\"statsmodels Beta std err = {}\".format(model.bse[1]))\n",
    "print(\"Reproduced  Beta std err = {}\".format(beta_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute the variance of $\\hat{\\alpha}$. First, note that we can express $\\hat{\\alpha} - \\alpha$ as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\alpha} - \\alpha\n",
    "    &= \\bar{y} - (\\hat{\\beta} - \\beta)\\bar{x} - \\beta\\bar{x} - \\alpha \\\\\n",
    "    &= (\\bar{y} - (\\beta\\bar{x} + \\alpha)) - (\\hat{\\beta} - \\beta)\\bar{x} \\\\\n",
    "    &= (\\bar{y} - \\mathbb{E}[\\bar{y}]) - (\\hat{\\beta} - \\beta)\\bar{x}.\n",
    "\\end{align*}\n",
    "\n",
    "To compute the variance of $\\hat{\\alpha}$, we will need the covariance between $\\hat{\\beta}$ and $\\bar{y}$. We can express $\\bar{y} - \\mathbb{E}[\\bar{y}]$ as\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{y} - \\mathbb{E}[\\bar{y}]\n",
    "    &= \\bar{y} - \\beta \\bar{x} - \\alpha \\\\\n",
    "    &= \\frac{1}{m}\\sum_{j=1}^m (y^{(j)} - \\beta x^{(j)} - \\alpha) \\\\\n",
    "    &= \\frac{1}{m}\\sum_{j=1}^m (y^{(j)} - \\mathbb{E}[y^{(j)}]),\n",
    "\\end{align*}\n",
    "\n",
    "from which it follows that\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Cov}[\\hat{\\beta}, \\bar{y}]\n",
    "    &= \\mathbb{E}\\left[(\\hat{\\beta} - \\beta) (\\bar{y} - \\mathbb{E}[\\bar{y}])\\right] \\\\\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x}) \\mathbb{E}\\left[(y^{(i)} - \\mathbb{E}[y^{(i)}]) (y^{(j)} - \\mathbb{E}[y^{(j)}])\\right] \\\\\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x}) \\text{Cov}[y^{(i)}, y^{(j)}] \\\\\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x}) \\sigma^2\\delta_{ij} \\\\\n",
    "    &= \\frac{\\sigma^2}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x}) \\\\\n",
    "    &= 0.\n",
    "\\end{align*}\n",
    "\n",
    "Also:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Var}[\\bar{y}]\n",
    "    &= \\mathbb{E}\\left[(\\bar{y} - \\mathbb{E}[\\bar{y}])^2\\right] \\\\\n",
    "    &= \\frac{1}{m^2}\\sum_{k,l=1}^m \\text{Cov}[y^{(k)}, y^{(l)}] \\\\\n",
    "    &= \\frac{1}{m^2}\\sum_{k,l=1}^m \\sigma^2\\delta_{kl} \\\\\n",
    "    &= \\frac{\\sigma^2}{m}.\n",
    "\\end{align*}\n",
    "\n",
    "This is exactly what we should've expected without doing any work, since $\\bar{y} - \\mathbb{E}[\\bar{y}] \\sim \\mathcal{N}(0, \\sigma^2/m)$ follows from the fact that $\\bar{y}$ is an average of independent and identical normally distributed random variables.\n",
    "\n",
    "Now we can compute the variance of $\\hat{\\alpha}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Var}[\\hat{\\alpha}]\n",
    "    &= \\mathbb{E}\\left[(\\hat{\\alpha} - \\alpha)^2\\right] \\\\\n",
    "    &= \\text{Var}[\\bar{y}] + \\bar{x}^2 \\text{Var}[\\hat{\\beta}] - 2\\bar{x}\\text{Cov}[\\hat{\\beta}, \\bar{y}] \\\\\n",
    "    &= \\frac{\\sigma^2}{m} + \\bar{x}^2\\frac{\\sigma^2}{mS_x^2} - 0 \\\\\n",
    "    &= \\frac{\\sigma^2}{m}(1+\\bar{x}^2/S_x^2).\n",
    "\\end{align*}\n",
    "\n",
    "Let's check this against `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Alpha std err = 0.113117048297\n",
      "Reproduced  Alpha std err = 0.113117048297\n"
     ]
    }
   ],
   "source": [
    "alpha_se = np.sqrt(model.scale / m * (1 + xmean**2/xvar))\n",
    "print(\"statsmodels Alpha std err = {}\".format(model.bse[0]))\n",
    "print(\"Reproduced  Alpha std err = {}\".format(alpha_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our calculation above also makes it easy to derive the covariance between $\\hat{\\alpha}$ and $\\hat{\\beta}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Cov}[\\hat{\\alpha}, \\hat{\\beta}]\n",
    "    &= \\mathbb{E}[(\\hat{\\alpha} - \\alpha)(\\hat{\\beta} - \\beta)] \\\\\n",
    "    &= \\text{Cov}[\\hat{\\beta}, \\bar{y}] - \\text{Var}[\\hat{\\beta}] \\\\\n",
    "    &= 0 - \\bar{x} \\frac{\\sigma^2}{mS_x^2} \\\\\n",
    "    &= -\\frac{\\sigma^2\\bar{x}}{mS_x^2}.\n",
    "\\end{align*}\n",
    "\n",
    "Again, we get the same number as `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels alpha-beta covariance = -0.0190967516823\n",
      "Reproduced  alpha-beta covariance = -0.0190967516823\n"
     ]
    }
   ],
   "source": [
    "cov_params = - model.scale * xmean/(m*xvar)\n",
    "print(\"statsmodels alpha-beta covariance = {}\".format(model.cov_params()[0, 1]))\n",
    "print(\"Reproduced  alpha-beta covariance = {}\".format(cov_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One detail we glossed over was how `statsmodels` estimates $\\sigma^2$. Let's think about this a little. Since, by assumption, $y{(i)} \\sim \\mathcal{N}(\\alpha + \\beta x^{(i)}, \\sigma^2)$, it makes sense to use the sample variance of the prediction error $e_i=y^{(i)} - \\hat{\\alpha} - \\hat{\\beta} x^{(i)}=y^{(i)} - f^{(i)}$ as an estimate. We can express $e_i$ as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "e_i &= y^{(i)} - \\hat{\\alpha} - \\hat{\\beta} x^{(i)} \\\\\n",
    "    &= (y^{(i)} - \\mathbb{E}[y^{(i)}]) - (\\hat{\\alpha} - \\alpha) - (\\hat{\\beta} - \\beta)x^{(i)} \\\\\n",
    "    &= (y^{(i)} - \\mathbb{E}[y^{(i)}]) - (\\bar{y} - \\mathbb{E}[\\bar{y}]) - (\\hat{\\beta} - \\beta)(x^{(i)} - \\bar{x}) \\\\\n",
    "    &= \\sum_{j=1}^{m}(y^{(j)} - \\mathbb{E}[y^{(j)}])\\left[\\delta_{ij} - \\frac{1}{m} - \\frac{1}{m S_x^2}(x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x}) \\right] \\\\\n",
    "    &= \\sum_{j=1}^{m}A_{ij}(y^{(j)} - \\mathbb{E}[y^{(j)}]),\n",
    "\\end{align*}\n",
    "\n",
    "where the second equality follows from $\\langle y^{(i)} \\rangle = \\alpha + \\beta x^{(i)}$, the third from $\\hat{\\alpha} - \\alpha = (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)\\bar{x}$, the fourth from our earlier expressions for $\\bar{y} - \\langle \\bar{y} \\rangle$ and $\\hat{\\beta} - \\beta$, and in the last equality we have introduced the matrix $A$ whose elements are given by:\n",
    "\n",
    "\\begin{equation}\n",
    "A_{ij} \\equiv \\delta_{ij} - \\frac{1}{m} - \\frac{1}{m S_x^2}(x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x}).\n",
    "\\end{equation}\n",
    "\n",
    "$A$ has a few interesting properties. First, it's symmetric: $A_{ij} = A_{ij}$. Second, it's equal to its square: $A^2=A$. (This can be shown by carrying out the matrix multiplication between $A$ and itself. It's a bit tedious and not terribly interesting, so I'll leave it out.) Using these results, we can compute the sum of squared errors:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{m} e_i^2\n",
    "    &= \\sum_{i=1}^{m}\\left[\\sum_{j=1}^{m}A_{ij}(y^{(j)} - \\mathbb{E}[y^{(j)}])\\right]^2 \\\\\n",
    "    &= \\sum_{j,k=1}^{m}(y^{(j)} - \\mathbb{E}[y^{(j)}])(y^{(k)} - \\mathbb{E}[y^{(k)}])\\left[\\sum_{i=1}^{m}A_{ij}A_{ik}\\right] \\\\\n",
    "    &= \\sum_{j,k=1}^{m}(y^{(j)} - \\mathbb{E}[y^{(j)}])(y^{(k)} - \\mathbb{E}[y^{(k)}]) A_{jk},\n",
    "\\end{align*}\n",
    "\n",
    "where the last equality follows from the fact that $A$ is symmetric and $A^2=A$. Taking the expectation value of both sides:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}\\left[\\sum_{i=1}^{m} e_i^2 \\right]\n",
    "    &= \\sum_{j,k=1}^{m} A_{jk}\\mathbb{E}\\left[(y^{(j)} - \\mathbb{E}[y^{(j)}])(y^{(k)} - \\mathbb{E}[y^{(k)}])\\right] \\\\\n",
    "    &= \\sum_{j,k=1}^{m} A_{jk}\\text{Cov}[y^{(j)}, y^{(k)}] \\\\\n",
    "    &= \\sum_{j,k=1}^{m} A_{jk}\\sigma^2\\delta_{ij} \\\\\n",
    "    &= \\sigma^2 \\text{tr}(A),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\text{tr}(A)=\\sum_{i=1}^{m}A_{ii}$ denotes the trace of $A$. It's easy to show that $\\text{tr}(A)=m-2$, so the unbiased estimator $\\hat{\\sigma}^2$ of $\\sigma^2$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\sigma}^2\n",
    "    &= \\frac{1}{m-2}\\sum_{i=1}^{m}e_i^2 \\\\\n",
    "    &= \\frac{1}{m-2}\\sum_{i=1}^m(y^{(i)} - \\hat{\\alpha} - \\hat{\\beta} x^{(i)})^2.\n",
    "\\end{align*}\n",
    "\n",
    "Let's see if this reproduces the estimate of `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels estimate of sigma^2 = 0.324709077426\n",
      "Reproduced  estimate of sigma^2 = 0.324709077426\n"
     ]
    }
   ],
   "source": [
    "yvar = np.dot(e, e)/(m-2)\n",
    "print(\"statsmodels estimate of sigma^2 = {}\".format(model.scale))\n",
    "print(\"Reproduced  estimate of sigma^2 = {}\".format(yvar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence intervals\n",
    "\n",
    "We've worked out the variance (and covariance) of the estimates $\\hat{\\alpha}$ and $\\hat{\\beta}$. It would also be nice to have confidence intervals for them, so we can make statements like: there's a $95\\%$ chance that the interval $(\\beta_L, \\beta_U)$ contains $\\beta$. We've done this sort of analysis for the sample mean when discussing the [t-distribution](https://github.com/siavashaslanbeigi/stats_notes/blob/master/t.ipynb). Can we do something similar for $\\hat{\\alpha}$ and $\\hat{\\beta}$? Yes, but it's a bit more involved than dealing with the sample mean.\n",
    "\n",
    "\n",
    "Above we showed that $\\hat{\\beta} \\sim \\mathcal{N}(\\beta, \\sigma^2/(m S_x^2))$, or equivalently $\\sqrt{m} S_x(\\hat{\\beta} - \\beta)/\\sigma \\sim \\mathcal{N}(0, 1)$. This means $\\beta_L=\\hat{\\beta}-z\\frac{\\sigma}{\\sqrt{m} S_x}$ and $\\beta_U=\\hat{\\beta}+z\\frac{\\sigma}{\\sqrt{m} S_x}$ define a valid confidence interval for confidence level $1 - \\gamma$, where $z=\\Phi^{-1}(1-\\gamma/2)$ and $\\Phi_t^{-1}$ is the inverse CDF of the standard normal distribution:\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\hat{\\beta}-z\\frac{\\sigma}{\\sqrt{m} S_x} \\le \\beta \\le \\hat{\\beta}+z \\frac{\\sigma}{\\sqrt{m} S_x})\n",
    "    &= P(-z \\le \\frac{\\hat{\\beta} - \\beta}{\\sigma/(\\sqrt{m} S_x)} \\le z ) \\\\\n",
    "    &= 2\\Phi(z)-1 \\\\\n",
    "    &= 2\\Phi(\\Phi^{-1}(1-\\gamma/2))-1 \\\\\n",
    "    &= 1-\\gamma.\n",
    "\\end{align*}\n",
    "\n",
    "The problem, though, is that we do not know $\\sigma^2$. What if we replace it with the estimate $\\hat{\\sigma}^2$? Then we'll need to know the distribution of\n",
    "\n",
    "\\begin{align*}\n",
    "T\n",
    "    &\\equiv \\frac{\\hat{\\beta} - \\beta}{\\sqrt{\\hat{\\sigma}^2 / (m S_x^2)}} \\\\\n",
    "    &= \\frac{\\sqrt{m} S_x(\\hat{\\beta} - \\beta)/\\sigma}{\\sqrt{\\frac{1}{(m-2)\\sigma^2}\\sum_{i=1}^{m}e_i^2}} \\\\\n",
    "    &= \\frac{Z}{\\sqrt{\\frac{V}{m-2}}},\n",
    "\\end{align*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "Z \\equiv \\frac{\\sqrt{m} S_x(\\hat{\\beta} - \\beta)}{\\sigma}, \\qquad\n",
    "V \\equiv \\frac{1}{\\sigma^2}\\sum_{i=1}^{m}e_i^2.\n",
    "\\end{equation}\n",
    "\n",
    "We already know that $Z \\sim \\mathcal{N}(0, 1)$. In what follows, we will show that $V$ has a chi-squared distribution with $m-2$ degrees of freedom and that $Z$ and $V$ are independent. Therefore, $T$ has a t-distribution with $m-2$ degrees of freedom.\n",
    "\n",
    "\n",
    "We will make use (one of the formulations of) [Cochran's theorem](https://en.wikipedia.org/wiki/Cochran%27s_theorem), which states the following: suppose $Y \\sim \\mathcal{N}(0, \\sigma^2 I)$ is an $m$-dimensional multivariate normal random vector, and that $B^{(1)}, \\dots, B^{(k)}$ are symmetric $m \\times m$ matrices which satisfy the following properties:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{k}B^{(i)} = I, \\qquad\n",
    "\\sum_{i=1}^{k}r_i = m,\n",
    "\\end{equation}\n",
    "where $I$ is the $m \\times m$ identity matrix and $r_i$ denotes the rank of $B_i$. Then, the random variable $Q^{(i)}=Y^TB^{(i)}Y$ is distributed as $\\sigma^2 \\chi^2_{r_i}$, where $\\chi^2_{r_i}$ is the chi-squared distribution with $r_i$ degrees of freedom, and $Q^{(i)}$ and $Q^{(j)}$ are independent for all $i\\neq j$.\n",
    "\n",
    "\n",
    "It's not immediately obvious how this theorem will help us arrive at confidence intervals, but it will. First, note that if we define $Y$ as follows\n",
    "\n",
    "\\begin{equation}\n",
    "    Y_i \\equiv y^{(i)} - \\mathbb{E}[y^{(i)}].\n",
    "\\end{equation}\n",
    "then $Y \\sim \\mathcal{N}(0, \\sigma^2 I)$. With this definition, we can rewrite the sum of squared errors as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{m}e_i^2 = Y^TAY.\n",
    "\\end{equation}\n",
    "\n",
    "Now this looks like a $Q$ matrix from Cochran's theorem. What is the rank of $A$? Remember that the rank of a matrix is the dimension of the vector space spanned by its column vectors. It can be shown that this is equal to the number of non-zero eigenvalues. Because $A^2=A$, eigenvalues of $A$ are either $1$ or $0$. To see why, let $v$ denote an eigenvector of $A$ with eigenvalue $\\lambda$: $Av=\\lambda v$. Then:\n",
    "\n",
    "\\begin{equation}\n",
    "\\lambda v = Av = A^2v=A(Av)=A(\\lambda v)=\\lambda Av=\\lambda^2v,\n",
    "\\end{equation}\n",
    "\n",
    "which implies $\\lambda^2=\\lambda$, which has the solutions $\\lambda=0$ and $\\lambda=1$. Therefore, for $A$, the number of non-zero eigenvalues is simply the sum of its eigenvalues, which is also its trace:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Rank}(A) = \\text{tr}(A) = m - 2.\n",
    "\\end{equation}\n",
    "Let's summarize what we have so far\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{(1)} \\equiv \\sum_{i=1}^{m}e_i^2 = Y^TB^{(1)}Y, \\qquad\n",
    "B^{(1)}_{ij} = \\delta_{ij} - \\frac{1}{m} - \\frac{1}{m S_x^2}(x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x}), \\qquad\n",
    "r_1 = m - 2.\n",
    "\\end{equation}\n",
    "\n",
    "Next we'll turn our attention to $\\hat{\\beta} - \\beta$. We had previous shown that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} - \\beta = \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\mathbb{E}[y^{(i)}]),\n",
    "\\end{equation}\n",
    "\n",
    "from which it follows that\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{(2)} \\equiv m S_x^2(\\hat{\\beta} - \\beta)^2 = Y^TB^{(2)}Y, \\qquad\n",
    "B^{(2)}_{ij} = \\frac{1}{m S_x^2}(x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x}).\n",
    "\\end{equation}\n",
    "\n",
    "Again, it can be checked that $B^{(2)}$ is equal to its own square, so its rank is given by its trace:\n",
    "\n",
    "\\begin{equation}\n",
    "r_2 = \\text{Rank}(B^{(2)}) = \\text{tr}(B^{(2)}) = \\sum_{i=1}^{m} B^{(2)}_{ii} = \\frac{1}{m S_x^2}\\sum_{i=1}^{m} (x^{(i)} - \\bar{x})^2 = 1.\n",
    "\\end{equation}\n",
    "\n",
    "Finally, as we showed in previous section\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{y} - \\mathbb{E}[\\bar{y}] = \\frac{1}{m}\\sum_{i=1}^m (y^{(i)} - \\mathbb{E}[y^{(i)}]),\n",
    "\\end{equation}\n",
    "\n",
    "from which it follows that\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{(3)} \\equiv m (\\bar{y} - \\mathbb{E}[\\bar{y}])^2 = Y^TB^{(3)}Y, \\qquad\n",
    "B^{(3)}_{ij} = \\frac{1}{m}.\n",
    "\\end{equation}\n",
    "\n",
    "Since all columns of $B^{(3)}$ are the same, its rank is $1$:\n",
    "\n",
    "\\begin{equation}\n",
    "r_3 = 1.\n",
    "\\end{equation}\n",
    "\n",
    "It's now easy to check that $B^{(1)}$, $B^{(2)}$, and $B^{(3)}$ satisfy the conditions of Cochran's theorem:\n",
    "\n",
    "\\begin{equation}\n",
    "B^{(1)}_{ij} + B^{(2)}_{ij} + B^{(3)}_{ij} = \\delta_{ij}, \\qquad\n",
    "r_1 + r_2 + r_3 = m.\n",
    "\\end{equation}\n",
    "\n",
    "Cochran's theorem then implies the following:\n",
    "\n",
    "* $\\sum_{i=1}^{m}e_i^2 \\sim \\sigma^2 \\chi^2_{m-2}$.\n",
    "* $\\sum_{i=1}^{m}e_i^2$, $(\\hat{\\beta} - \\beta)^2$, and $(\\bar{y} - \\langle \\bar{y} \\rangle)^2$ are mutually independent, which implies $\\sum_{i=1}^{m}e_i^2$, $\\hat{\\beta} - \\beta$, and $\\bar{y} - \\langle \\bar{y} \\rangle$ are mutually independent.\n",
    "\n",
    "This in turn implies that $V$ has a $\\chi^2_{m-2}$ distribution and that $Z$ and $V$ are independent. Therefore, \n",
    "\n",
    "\\begin{equation}\n",
    "T = \\frac{Z}{\\sqrt{\\frac{V}{m-2}}} = \\frac{\\hat{\\beta} - \\beta}{s_{\\hat{\\beta}}}\n",
    "\\end{equation}\n",
    "\n",
    "has the t-distribution with $m-2$ degrees of freedom, where\n",
    "\n",
    "\\begin{equation}\n",
    "s_{\\hat{\\beta}}^2 = \\frac{\\hat{\\sigma}^2}{m S_x^2}.\n",
    "\\end{equation}\n",
    "\n",
    "Finally: $\\beta_L=\\hat{\\beta}-ts_{\\hat{\\beta}}$ and $\\beta_U=\\hat{\\beta}+ts_{\\hat{\\beta}}$ define a valid confidence interval for confidence level $1-\\gamma$ ($0\\le \\gamma \\le 1$), where $t=\\Phi_t^{-1}(1-\\gamma/2)$ and $\\Phi_t^{-1}$ is the inverse CDF of the t-distribution with $m-2$ degrees of freedom. This gives us the same confidence interval as `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels beta 95% confidence interval = (1.62620621534, 2.40186144467)\n",
      "Reproduced  beta 95% confidence interval = (1.62620621534, 2.40186144467)\n"
     ]
    }
   ],
   "source": [
    "# statsmodel confidence interval\n",
    "statsmodels_ci = model.conf_int()\n",
    "t = stats.t.ppf(1 - 0.05/2., m - 2)\n",
    "# Reproduced confidence interval for beta\n",
    "beta_se = np.sqrt(yvar / (m * np.var(x)))\n",
    "print(\"statsmodels beta 95% confidence interval = ({0}, {1})\".format(statsmodels_ci[1, 0], statsmodels_ci[1, 1]))\n",
    "print(\"Reproduced  beta 95% confidence interval = ({0}, {1})\".format(beta - t*beta_se, beta + t*beta_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the confidence interval for $\\hat{\\alpha}$. Earlier we had shown that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\alpha} \\sim \\mathcal{N}\\left(\\alpha, \\frac{\\sigma^2}{m}(1+\\bar{x}^2/S_x^2)\\right).\n",
    "\\end{equation}\n",
    "\n",
    "As before, we divide $\\hat{\\alpha} - \\alpha$ by its standard deviation and replace $\\sigma$ with $\\hat{\\sigma}$:\n",
    "\n",
    "\\begin{align*}\n",
    "T_{\\alpha}\n",
    "    &\\equiv \\frac{\\hat{\\alpha} - \\alpha}{s_{\\hat{\\alpha}}} \\\\\n",
    "    &= \\frac{\\hat{\\alpha} - \\alpha}{\\sqrt{\\frac{\\hat{\\sigma}^2}{m}(1+\\bar{x}^2/S_x^2)}} \\\\\n",
    "    &= \\frac{\\sqrt{m}(\\hat{\\alpha} - \\alpha)/\\sqrt{\\sigma^2(1+\\bar{x}^2/S_x^2)}}{\\sqrt{\\frac{1}{(m-2)\\sigma^2}\\sum_{i=1}^{m}e_i^2}} \\\\\n",
    "    &= \\frac{Z_{\\alpha}}{\\sqrt{\\frac{V}{m-2}}},\n",
    "\\end{align*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "s_{\\hat{\\alpha}} \\equiv \\frac{\\hat{\\sigma}^2}{m}(1+\\bar{x}^2/S_x^2), \\qquad\n",
    "Z_{\\alpha} = \\frac{\\hat{\\alpha} - \\alpha}{\\sqrt{\\frac{\\sigma^2}{m}(1+\\bar{x}^2/S_x^2)}}.\n",
    "\\end{equation}\n",
    "\n",
    "We know that $Z_{\\alpha} \\sim \\mathcal{N}(0, 1)$ and $V \\sim \\chi^2_{m-2}$. If $Z_{\\alpha}$ and $V$ are independent, then $T_{\\alpha}$ also has a t-distribution with $m-2$ degrees of freedom. We had shown earlier that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\alpha} - \\alpha = (\\bar{y} - \\mathbb{E}[\\bar{y}]) - (\\hat{\\beta} - \\beta)\\bar{x}.\n",
    "\\end{equation}\n",
    "\n",
    "Since $\\sum_{i=1}^{m}e_i^2$, $\\hat{\\beta}$, and $\\bar{y}$ are independent, it follows that $\\hat{\\alpha}$ and $\\sum_{i=1}^{m}e_i^2$, and in turn $Z_{\\alpha}$ and $V$, are independent. As a result, $\\alpha_L=\\hat{\\alpha}-ts_{\\hat{\\alpha}}$ and $\\alpha_U=\\hat{\\alpha}+ts_{\\hat{\\alpha}}$ define a valid confidence interval for confidence level $1-\\gamma$ ($0\\le \\gamma \\le 1$), where $t=\\Phi_t^{-1}(1-\\gamma/2)$ and $\\Phi_t^{-1}$ is the inverse CDF of the t-distribution with $m-2$ degrees of freedom. This gives us the same confidence interval as `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels alpha 95% confidence interval = (0.082060520856, 0.531014722628)\n",
      "Reproduced  alpha 95% confidence interval = (0.082060520856, 0.531014722628)\n"
     ]
    }
   ],
   "source": [
    "alpha_se = np.sqrt(yvar / m * (1 + xmean**2/xvar))\n",
    "print(\"statsmodels alpha 95% confidence interval = ({0}, {1})\".format(statsmodels_ci[0, 0], statsmodels_ci[0, 1]))\n",
    "print(\"Reproduced  alpha 95% confidence interval = ({0}, {1})\".format(alpha - t*alpha_se, alpha + t*alpha_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict using $\\hat{\\alpha}$ and $\\hat{\\beta}$, so our predictions are also subject to statistical noise. Let's compute the variance of $\\hat{f}=\\hat{\\alpha} + \\hat{\\beta} x$. First, its mean is what we expect it to be:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[\\hat{f}]\n",
    "    &= \\mathbb{E}[\\hat{\\alpha}] + \\mathbb{E}[\\hat{\\beta}] x \\\\\n",
    "    &= \\alpha + \\beta x.\n",
    "\\end{align*}\n",
    "\n",
    "As a result:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{f} - \\mathbb{E}[\\hat{f}]\n",
    "    &= (\\hat{\\alpha} - \\alpha) + (\\hat{\\beta} - \\beta)x \\\\\n",
    "    &= (\\bar{y} - \\mathbb{E}[\\bar{y}]) - (\\hat{\\beta} - \\beta)\\bar{x}  + (\\hat{\\beta} - \\beta)x \\\\\n",
    "    &= (\\bar{y} - \\mathbb{E}[\\bar{y}]) + (\\hat{\\beta} - \\beta)(x - \\bar{x}),\n",
    "\\end{align*}\n",
    "\n",
    "where the second equality follows from $\\hat{\\alpha} - \\alpha = (\\bar{y} - \\mathbb{E}[\\bar{y}]) - (\\hat{\\beta} - \\beta)\\bar{x}$. Since $\\bar{y}$ and $\\hat{\\beta}$ are independent, the variance of $\\hat{f}$ is given simply by:\n",
    "\n",
    "The variance is given by:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Var}[\\hat{f}]\n",
    "    &= \\mathbb{E}\\left[(\\hat{f} - \\mathbb{E}[\\hat{f}])^2 \\right] \\\\\n",
    "    &= \\text{Var}[\\bar{y}] + (x - \\bar{x})^2 \\text{Var}[\\hat{\\beta}] + 2(x - \\bar{x})\\text{Cov}[\\hat{\\beta}, \\bar{y}] \\\\\n",
    "    &= \\frac{\\sigma^2}{m} + (x - \\bar{x})^2 \\frac{\\sigma^2}{m S_x^2} \\\\\n",
    "    &= \\frac{\\sigma^2}{m}\\left[1+\\frac{(x - \\bar{x})^2}{S_x^2}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "The same argument we used for deriving the confidence interval of $\\hat{\\alpha}$ can be used for $\\hat{f}$: $f_L=\\hat{f}-ts_{\\hat{f}}$ and $f_U=\\hat{f}+ts_{\\hat{f}}$ define a valid confidence interval for confidence level $1-\\gamma$ ($0\\le \\gamma \\le 1$), where $t=\\Phi_t^{-1}(1-\\gamma/2)$, $\\Phi_t^{-1}$ is the inverse CDF of the t-distribution with $m-2$ degrees of freedom, and:\n",
    "\n",
    "\\begin{equation}\n",
    "s_{\\hat{f}}^2 = \\frac{\\hat{\\sigma}^2}{m}\\left[1+\\frac{(x - \\bar{x})^2}{S_x^2}\\right].\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We can use `seaborn.regplot` to plot the confidence interval for $f$. Below we also show the $95\\%$ confidence interval computed using the formula we just derived:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11263f0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard error of f\n",
    "x_axis = np.linspace(0, 1, 1000)\n",
    "f_se = np.sqrt(yvar/m*(1 + (x_axis - xmean)**2/xvar))\n",
    "f_l = alpha + beta * x_axis - t * f_se\n",
    "f_u = alpha + beta * x_axis + t * f_se\n",
    "\n",
    "# Plot\n",
    "plt.plot(x_axis, f_l, 'r--')\n",
    "plt.plot(x_axis, f_u, 'r--', label='Reproduced 95% confidence interval')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "# Plot using seaborn\n",
    "seaborn.regplot(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand the quantities $t$ and $P>|t|$ which are quoted for both $\\hat{\\alpha}$ and $\\hat{\\beta}$ in the regression summary above. Consider the following null hypothesis: $\\beta=0$. In other words, we assume that the true population value of $\\beta$ is zero. Given this assumption, we can compute a concrete $t$-value for $\\hat{\\beta}$: $t=\\hat{\\beta}/s_{\\hat{\\beta}}$. Futhermore, we can calculate the probability of observing a value greater than $|t|$, given the null hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels beta t-value = 10.3055698889\n",
      "Reproduced  beta t-value = 10.3055698889\n",
      "\n",
      "statsmodels beta p-value = 2.63092577535e-17\n",
      "Reproduced  beta p-value = 2.63092577535e-17\n"
     ]
    }
   ],
   "source": [
    "tval_beta = beta/beta_se\n",
    "pval_beta = 2*stats.t.cdf(-abs(tval_beta), m - 2)\n",
    "\n",
    "print(\"statsmodels beta t-value = {0}\".format(model.tvalues[1]))\n",
    "print(\"Reproduced  beta t-value = {0}\".format(tval_beta))\n",
    "print(\"\")\n",
    "print(\"statsmodels beta p-value = {0}\".format(model.pvalues[1]))\n",
    "print(\"Reproduced  beta p-value = {0}\".format(pval_beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for $\\hat{\\alpha}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels beta t-value = 2.70991531654\n",
      "Reproduced  beta t-value = 2.70991531654\n",
      "\n",
      "statsmodels beta p-value = 0.00794562387443\n",
      "Reproduced  beta p-value = 0.00794562387443\n"
     ]
    }
   ],
   "source": [
    "tval_alpha = alpha/alpha_se\n",
    "pval_alpha = 2*stats.t.cdf(-abs(tval_alpha), m - 2)\n",
    "\n",
    "print(\"statsmodels beta t-value = {0}\".format(model.tvalues[0]))\n",
    "print(\"Reproduced  beta t-value = {0}\".format(tval_alpha))\n",
    "print(\"\")\n",
    "print(\"statsmodels beta p-value = {0}\".format(model.pvalues[0]))\n",
    "print(\"Reproduced  beta p-value = {0}\".format(pval_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual statistics\n",
    "\n",
    "Let's reproduce some of the statistics around the residuals here.\n",
    "We've covered [skewness](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/skewtest.ipynb), [kurtosis](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/kurtosistest.ipynb), and [Jarque-Bera](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/jarque_bera.ipynb) in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness         = 0.0350869362001\n",
      "Kurtosis         = 2.35646285462\n",
      "Jarque-Bera (JB) = 1.74610179106\n",
      "Prob(JB)         = 0.417675319649\n"
     ]
    }
   ],
   "source": [
    "jb_val, jb_p = stats.jarque_bera(e)\n",
    "\n",
    "print(\"Skewness         = {0}\".format(stats.skew(e)))\n",
    "print(\"Kurtosis         = {0}\".format(stats.kurtosis(e) + 3))\n",
    "print(\"Jarque-Bera (JB) = {0}\".format(jb_val))\n",
    "print(\"Prob(JB)         = {0}\".format(jb_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Prob(JB) is measuring the probability of obtaining a JB value more than what we get from our sample, under the null hypothesis that the residual population is normally distributed. As discussed here [Jarque-Bera](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/jarque_bera.ipynb), the implementation of JB actually uses the $\\chi^2_2$ distribution, which is only a good approximation for large samples ($m \\gtrsim 2000$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41767531964923776"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - stats.chi2.cdf(jb_val, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
