{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with multiple variables\n",
    "\n",
    "The goal of this notebook is to generalize [linear regression with one variable](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/linreg.ipynb) (also called *simple* linear regression) to multiple variables. The model is a natural extension to $n$ independent variables (or *features*):\n",
    "\n",
    "\\begin{equation}\n",
    "f(x_1, x_2, \\dots, x_n) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n.\n",
    "\\end{equation}\n",
    "\n",
    "A single observation corresponds to a value for each $x_1, x_2, \\dots, x_n$, as well as the dependent variable $y$. We will assume there are $m$ such observations. Let's start by establishing notation:\n",
    "\n",
    "* $y^{(i)}$: value of the dependent variable for the $i^{\\text{th}}$ observation.\n",
    "* $x^{(i)}_{j}$: value of the $j^{\\text{th}}$ feature for the $i^{\\text{th}}$ observation.\n",
    "* $x_0 \\equiv 1$. In other words, $x^{(i)}_{0} = 1$ for all $i=1,\\dots,m$.\n",
    "* $x=\\begin{pmatrix}x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_n\\end{pmatrix} \\in \\mathbb{R}^{n+1}$ denotes the vector of features.\n",
    "* $x^{(i)}=\\begin{pmatrix}x^{(i)}_0 \\\\ x^{(i)}_1 \\\\ \\vdots \\\\ x^{(i)}_n\\end{pmatrix} \\in \\mathbb{R}^{n+1}$ denotes the vector of features for the $i^{\\text{th}}$ observation.\n",
    "* $\\beta=\\begin{pmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_n\\end{pmatrix} \\in \\mathbb{R}^{n+1}$ denotes the vector of parameters.\n",
    "* $X_{ij} \\equiv x^{(i)}_{j}$ for $i=1,\\dots,m$ and $j=0,\\dots,n$. $X$ is called the *design matrix*. It's an $m \\times (n+1)$ dimensional matrix where each row contains the value of features for a given observation. Note that the first column is just a vector of $1$s, since by definition $x^{(i)}_{0} = 1$.\n",
    "* $Y_i \\equiv y^{(i)}$: $m$-dimensional vector created from $y^{(1)}, \\dots, y^{(m)}$. It's sometimes called the *target vector*.\n",
    "\n",
    "Expressing the data in vectorized form will make our life much easier. For instance, $f$ can now be expressed as a dot product:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\beta^Tx.\n",
    "\\end{equation}\n",
    "\n",
    "We will use the same cost function as we did for simple linear regression, i.e. the normalized sum of squared prediction errors:\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\beta) &= \\frac{1}{2m}\\sum_{i=1}^{m} (f(x^{(i)}) - y^{(i)})^2 \\\\\n",
    "         &= \\frac{1}{2m}\\sum_{i=1}^{m} (\\beta^Tx^{(i)} - y^{(i)})^2.\n",
    "\\end{align*}\n",
    "\n",
    "Note that\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta^Tx^{(i)} = \\sum_{j=0}^{n} \\beta_j x^{(i)}_{j} = \\sum_{j=0}^{n} X_{ij}\\beta_j = [X\\beta]_{i}.\n",
    "\\end{equation}\n",
    "\n",
    "This allows us to express $J$ in matrix form:\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\beta) &= \\frac{1}{2m}\\sum_{i=1}^{m} (\\beta^Tx^{(i)} - y^{(i)})^2 \\\\\n",
    "         &= \\frac{1}{2m}\\sum_{i=1}^{m} ([X\\beta]_{i} - Y_i)^2 \\\\\n",
    "         &= \\frac{1}{2m}\\sum_{i=1}^{m} [X\\beta - Y]_{i}[X\\beta - Y]_{i} \\\\\n",
    "         &= \\frac{1}{2m}[X\\beta - Y]^T[X\\beta - Y].\n",
    "\\end{align*}\n",
    "\n",
    "To find the parameters $\\beta_0,\\beta_1, \\dots, \\beta_n$ that best fit the data set, we will minimize $J$ with respect to each one of them. To do that, first we need the gradient of $J$ with respect to each $\\beta_j$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial \\beta_j}\n",
    "    &= \\frac{1}{m}\\sum_{i=1}^{m}(\\beta^Tx^{(i)} - y^{(i)})x^{(i)}_{j} \\\\\n",
    "    &= \\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[X\\theta - Y]_{i} \\\\\n",
    "    &= \\frac{1}{m} [X^T(X\\beta - Y)]_j.\n",
    "\\end{align*}\n",
    "\n",
    "Setting the gradients to zero:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla J = 0 \\to X^T(X\\hat{\\beta} - Y) = 0,\n",
    "\\end{equation}\n",
    "\n",
    "we find that the optimal parameters $\\hat{\\beta}$ are given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} = (X^TX)^{-1}X^TY.\n",
    "\\end{equation}\n",
    "\n",
    "This is called the *normal equation*. Does it reproduce the formulas we found for $\\hat{\\alpha}$ and $\\hat{\\beta}$ in [simple linear regression](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/linreg.ipynb)? Let's check. First note the mapping between the variables:\n",
    "\n",
    "| Multiple LR     | Simple LR       |\n",
    "|-----------------|-----------------|\n",
    "| $x^{(i)}_1$     | $x^{(i)}$       |\n",
    "| $\\hat{\\beta}_0$ | $\\hat{\\alpha}$  |\n",
    "| $\\hat{\\beta}_1$ | $\\hat{\\beta}$   |\n",
    "\n",
    "Using this, it can be shown after straightforward algebra that (see [this](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/linreg.ipynb) notebook for undefined quantities below)\n",
    "\\begin{equation}\n",
    "X^TX = m\n",
    "\\begin{pmatrix}\n",
    "    1       & \\bar{x} \\\\\n",
    "    \\bar{x} & S_x^2 + \\bar{x}^2\n",
    "\\end{pmatrix}, \\qquad\n",
    "X^TY = m\n",
    "\\begin{pmatrix}\n",
    "    \\bar{y} \\\\\n",
    "    C_{xy} + \\bar{x}\\bar{y}\n",
    "\\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "The inverse of $X^TX$ is given by\n",
    "\\begin{equation}\n",
    "(X^TX)^{-1} = \\frac{1}{mS_x^2}\n",
    "\\begin{pmatrix}\n",
    "    S_x^2 + \\bar{x}^2       & -\\bar{x} \\\\\n",
    "    -\\bar{x} & 1\n",
    "\\end{pmatrix},\n",
    "\\end{equation}\n",
    "\n",
    "using which we can compute $\\hat{\\beta}$: \n",
    "\\begin{equation}\n",
    "\\hat{\\beta}\n",
    "    = (X^TX)^{-1}X^TY\n",
    "    = \\frac{1}{S_x^2}\n",
    "        \\begin{pmatrix} S_x^2 + \\bar{x}^2 & -\\bar{x} \\\\ -\\bar{x} & 1 \\end{pmatrix}\n",
    "        \\begin{pmatrix} \\bar{y} \\\\ C_{xy} + \\bar{x}\\bar{y} \\end{pmatrix}\n",
    "    = \\frac{1}{S_x^2}\n",
    "    \\begin{pmatrix} \\bar{y}S_x^2 -\\bar{x}C_{xy} \\\\ C_{xy} \\end{pmatrix}\n",
    "    = \\begin{pmatrix} \\bar{y} -\\bar{x}\\frac{C_{xy}}{S_x^2} \\\\ \\frac{C_{xy}}{S_x^2} \\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "That's exactly what we had derived previously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression with `statsmodels`\n",
    "\n",
    "Below we will create a fake dataset, run linear regression on it using `statsmodels`, and try to reproduce the various outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels import regression\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate data using the model\n",
    "\n",
    "\\begin{equation}\n",
    "y(x_1, x_2) \\sim \\mathcal{N}(0.3 + 2x_1 + 5 x_2, 0.5^2).\n",
    "\\end{equation}\n",
    "\n",
    "So, the \"true\" values of the parameters we will be estimating are $\\beta_0 = 0.3$, $\\beta_1 = 2$, and $\\beta_2 = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make results reproducible\n",
    "np.random.seed(123)\n",
    "\n",
    "m = 100\n",
    "x1 = np.random.uniform(size=m)\n",
    "x2 = np.random.uniform(size=m)\n",
    "y = np.random.normal(loc=0.3 + 2.0*x1 + 5.0*x2, scale=0.5, size=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the data using `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.906</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.904</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   468.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 09 Mar 2019</td> <th>  Prob (F-statistic):</th> <td>1.37e-50</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:39:06</td>     <th>  Log-Likelihood:    </th> <td> -65.432</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   136.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    97</td>      <th>  BIC:               </th> <td>   144.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.3851</td> <td>    0.142</td> <td>    2.706</td> <td> 0.008</td> <td>    0.103</td> <td>    0.668</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    1.7702</td> <td>    0.193</td> <td>    9.150</td> <td> 0.000</td> <td>    1.386</td> <td>    2.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    4.9949</td> <td>    0.167</td> <td>   29.947</td> <td> 0.000</td> <td>    4.664</td> <td>    5.326</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 6.909</td> <th>  Durbin-Watson:     </th> <td>   1.675</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.032</td> <th>  Jarque-Bera (JB):  </th> <td>   9.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.299</td> <th>  Prob(JB):          </th> <td> 0.00973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.366</td> <th>  Cond. No.          </th> <td>    6.17</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.906\n",
       "Model:                            OLS   Adj. R-squared:                  0.904\n",
       "Method:                 Least Squares   F-statistic:                     468.9\n",
       "Date:                Sat, 09 Mar 2019   Prob (F-statistic):           1.37e-50\n",
       "Time:                        20:39:06   Log-Likelihood:                -65.432\n",
       "No. Observations:                 100   AIC:                             136.9\n",
       "Df Residuals:                      97   BIC:                             144.7\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.3851      0.142      2.706      0.008       0.103       0.668\n",
       "x1             1.7702      0.193      9.150      0.000       1.386       2.154\n",
       "x2             4.9949      0.167     29.947      0.000       4.664       5.326\n",
       "==============================================================================\n",
       "Omnibus:                        6.909   Durbin-Watson:                   1.675\n",
       "Prob(Omnibus):                  0.032   Jarque-Bera (JB):                9.264\n",
       "Skew:                          -0.299   Prob(JB):                      0.00973\n",
       "Kurtosis:                       4.366   Cond. No.                         6.17\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.column_stack([np.ones(m), x1, x2])\n",
    "_, p = X.shape\n",
    "model = regression.linear_model.OLS(y, X).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal parameters\n",
    "\n",
    "Let's compare the normal equation to the output of `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Beta_0 = 0.385073881382\n",
      "Reproduced  Beta_0 = 0.385073881382\n",
      "\n",
      "statsmodels Beta_1 = 1.77016602447\n",
      "Reproduced  Beta_1 = 1.77016602447\n",
      "\n",
      "statsmodels Beta_2 = 4.99486380715\n",
      "Reproduced  Beta_2 = 4.99486380715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "for i in range(len(beta)):\n",
    "    print(\"statsmodels Beta_{0} = {1}\".format(i, model.params[i]))\n",
    "    print(\"Reproduced  Beta_{0} = {1}\".format(i, beta[i]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-squared\n",
    "\n",
    "We introduced $R^2$ [here](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/linreg.ipynb). It's defined by\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^m(y^{(i)} - f^{(i)})^2}{\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2},\n",
    "\\end{equation}\n",
    "\n",
    "where $f(x)$ is any model of the data and\n",
    "\n",
    "\\begin{equation}\n",
    "f^{(i)} = f(x^{(i)}), \\qquad\n",
    "\\bar{y} = \\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}.\n",
    "\\end{equation}\n",
    "\n",
    "In the case of multiple linear regression:\n",
    "\n",
    "\\begin{equation}\n",
    "f^{(i)} = \\hat{\\beta}^Tx^{(i)} = [X\\hat{\\beta}]_i.\n",
    "\\end{equation}\n",
    "\n",
    "If we let $\\hat{f}$ define the vector of all preditions on input data:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f} \\equiv \\begin{pmatrix}f^{(1)} \\\\ f^{(2)} \\\\ \\vdots \\\\ f^{(m)} \\end{pmatrix},\n",
    "\\end{equation}\n",
    "\n",
    "then\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f} = X\\hat{\\beta}.\n",
    "\\end{equation}\n",
    "\n",
    "Let $e$ denote the vector of all prediction errors (or *residuals*) on input data\n",
    "\n",
    "\\begin{equation}\n",
    "e^{(i)} = y^{(i)} - f^{(i)}, \\qquad\n",
    "\\hat{e} \\equiv \\begin{pmatrix}e^{(1)} \\\\ e^{(2)} \\\\ \\vdots \\\\ e^{(m)} \\end{pmatrix} = Y - \\hat{f} = Y - X\\hat{\\beta}.\n",
    "\\end{equation}\n",
    "\n",
    "`statsmodels` stores the residuals in the attribute `resid`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels first five residuals = [-0.29277296  0.02056819  0.16915124  0.77827397  0.2356304 ]\n",
      "Reproduced  first five residuals = [-0.29277296  0.02056819  0.16915124  0.77827397  0.2356304 ]\n"
     ]
    }
   ],
   "source": [
    "f = X.dot(beta)\n",
    "e = y - f\n",
    "print(\"statsmodels first five residuals = {}\".format(model.resid[:5]))\n",
    "print(\"Reproduced  first five residuals = {}\".format(e[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute $R^2$ from scratch and compare the result with that of `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels R-squared = 0.906264829007\n",
      "Reproduced  R-squared = 0.906264829007\n"
     ]
    }
   ],
   "source": [
    "ymean = y.mean()\n",
    "\n",
    "rsquared = 1 - np.dot(e, e) / np.dot(y - ymean, y - ymean)\n",
    "print(\"statsmodels R-squared = {}\".format(model.rsquared))\n",
    "print(\"Reproduced  R-squared = {}\".format(rsquared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like simple linear regression, $R^2$ is equal to the square of the sample correlation between $f$ and $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9062648290073423"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(f, y)[0, 1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proved this property mathematically for simple linear regression. In what follows, we will generalize the proof to multiple linear regression.\n",
    "\n",
    "Recall that minimizing the cost function gave us the following equation: $X^T(X\\hat{\\beta} - Y)=0$, which can also be expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "X^T\\hat{e} = 0.\n",
    "\\end{equation}\n",
    "\n",
    "Looking at the first component of this equation:\n",
    "\n",
    "\\begin{equation}\n",
    "[X^T\\hat{e}]_0 = \\sum_{i=1}^{m}X_{i0}e^{(i)} = \\sum_{i=1}^{m}x^{(i)}_0e^{(i)} = \\sum_{i=1}^{m}e^{(i)} = 0,\n",
    "\\end{equation}\n",
    "\n",
    "we see that the sum of the residuals is zero. Since $e^{(i)}=y^{(i)} - f^{(i)}$, this implies\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{m}y^{(i)} = \\sum_{i=1}^{m} f^{(i)},\n",
    "\\end{equation}\n",
    "\n",
    "or equivalently\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{f} = \\bar{y},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{f} \\equiv \\frac{1}{m} \\sum_{i=1}^{m} f^{(i)}.\n",
    "\\end{equation}\n",
    "\n",
    "Also, if we apply $\\hat{\\beta}$ to both sides of $X^T\\hat{e} = 0$, we find:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}^TX^T\\hat{e} = (X\\hat{\\beta})^T\\hat{e} = \\hat{f}^T\\hat{e} = 0.\n",
    "\\end{equation}\n",
    "\n",
    "Alright, on to computing $R^2$. Let's start with the denominator:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{m}(y^{(i)} - \\bar{y})^2\n",
    "    &= \\sum_{i=1}^{m}[(y^{(i)} - f^{(i)}) + (f^{(i)} - \\bar{y})]^2 \\\\\n",
    "    &= \\sum_{i=1}^{m}[e^{(i)} + (f^{(i)} - \\bar{y})]^2 \\\\\n",
    "    &= \\sum_{i=1}^{m}(e^{(i)})^2 + \\sum_{i=1}^{m}(f^{(i)} - \\bar{y})^2 + 2\\sum_{i=1}^{m}(f^{(i)} - \\bar{y})e^{(i)} \\\\\n",
    "    &= \\sum_{i=1}^{m}(e^{(i)})^2 + \\sum_{i=1}^{m}(f^{(i)} - \\bar{y})^2 + 2\\hat{f}^T\\hat{e} - 2\\bar{y}\\sum_{i=1}^{m}e^{(i)} \\\\\n",
    "    &= \\sum_{i=1}^{m}(e^{(i)})^2 + \\sum_{i=1}^{m}(f^{(i)} - \\bar{y})^2 \\\\\n",
    "    &= \\sum_{i=1}^{m}(e^{(i)})^2 + \\sum_{i=1}^{m}(f^{(i)} - \\bar{f})^2\n",
    "\\end{align*}\n",
    "\n",
    "where we have used $\\sum_{i=1}^{m}e^{(i)}=0$ and $\\hat{f}^T\\hat{e}=0$ in the second last equality, and $\\bar{y}=\\bar{f}$ in the last one. Plugging this back into the definition of $R^2$, we find:\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = \\frac{\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2 - \\sum_{i=1}^m(e^{(i)})^2}{\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2}\n",
    "    = \\frac{\\sum_{i=1}^{m}(f^{(i)} - \\bar{f})^2}{\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2}.\n",
    "\\end{equation}\n",
    "\n",
    "The correlation $r_{fy}$ between $f$ and $y$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "r_{fy} = \\frac{C_{fy}}{S_fS_y},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "S_y = \\frac{1}{m}\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2, \\qquad\n",
    "S_f = \\frac{1}{m}\\sum_{i=1}^m (f^{(i)} - \\bar{f})^2, \\qquad\n",
    "C_{fy} = \\frac{1}{m}\\sum_{i=1}^m (f^{(i)} - \\bar{f})(y^{(i)} - \\bar{y}).\n",
    "\\end{equation}\n",
    "\n",
    "Given these definitions, we can also rewrite $R^2$ more simply as\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = \\frac{S_f^2}{S_y^2}.\n",
    "\\end{equation}\n",
    "\n",
    "The covariance between $f$ and $y$ can be further simplified:\n",
    "\n",
    "\\begin{align*}\n",
    "C_{fy}\n",
    "    &= \\frac{1}{m}\\sum_{i=1}^{m} (f^{(i)} - \\bar{f})(y^{(i)} - \\bar{y}) \\\\\n",
    "    &= \\frac{1}{m}\\sum_{i=1}^{m} (f^{(i)} - \\bar{f})(e^{(i)} + f^{(i)} - \\bar{f}) \\\\\n",
    "    &= \\frac{1}{m}\\sum_{i=1}^{m} (f^{(i)} - \\bar{f})^2 + \\frac{1}{m}\\sum_{i=1}^{m} f^{(i)}e^{(i)} - \\frac{\\bar{f}}{m}\\sum_{i=1}^{m}e^{(i)} \\\\\n",
    "    &= S_f^2,\n",
    "\\end{align*}\n",
    "\n",
    "where the second line uses the definition of $e^{(i)}$ and the fact that $\\bar{y}=\\bar{f}$, and the last equality follows from $\\sum_{i=1}^{m}e^{(i)}=0$ and $\\hat{f}^T\\hat{e}=0$.  Plugging this back in $r_{fy}$:\n",
    "\n",
    "\\begin{equation}\n",
    "r_{fy}^2 = \\frac{C_{fy}^2}{S_f^2S_y^2} = \\frac{S_f^4}{S_f^2S_y^2} = \\frac{S_f^2}{S_y^2} = R^2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic view\n",
    "\n",
    "The probabilistic view is much the same as before. We will assume $y^{(i)}$ are independent normally-distributed samples with standard deviation $\\sigma$: \n",
    "\n",
    "\\begin{equation}\n",
    "y^{(i)} \\sim \\mathcal{N}(\\beta^Tx^{(i)}, \\sigma^2)\n",
    "\\end{equation}\n",
    "\n",
    "The probabiliy of observing the dataset $(x^{(1)}, y^{(1)}), \\dots, (x^{(m)}, y^{(m)})$ is\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\beta, \\sigma^2) = \\frac{1}{(2\\pi \\sigma^2)^{m/2}}\\exp{\\left[-\\sum_{i=1}^m\\frac{(y^{(i)} - \\beta^Tx^{(i)})^2}{2\\sigma^2}\\right]} = \\frac{1}{(2\\pi \\sigma^2)^{m/2}}\\exp{\\left(-\\frac{mJ(\\beta)}{\\sigma^2}\\right)}.\n",
    "\\end{equation}\n",
    "\n",
    "As before, the parameters $\\hat{\\beta}$ that maximize $P$ are precisely those that minimize the cost function $J(\\beta)$. Also, we can reproduce `statsmodels`'s computation of log-likelihood by evaluating $\\log P(\\hat{\\beta}, \\tilde{\\sigma}^2)$, where $\\tilde{\\sigma}^2$ is the unbiased sample estimate of the variance of residuals:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\sigma}^2 = \\frac{1}{m}\\sum_{i=1}^{m}(e^{(i)})^2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Log-Likelihood = -65.4317634738\n",
      "Reproduced  Log-Likelihood = -65.4317634738\n"
     ]
    }
   ],
   "source": [
    "loglikelihood = -(m/2.0)*np.log(2*np.pi*np.var(e)) - m/2.0\n",
    "print(\"statsmodels Log-Likelihood = {}\".format(model.llf))\n",
    "print(\"Reproduced  Log-Likelihood = {}\".format(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error of estimates\n",
    "\n",
    "What is the probability distribution of $\\hat{\\beta}$? By definition, $Y$ has a multivariate Gaussian distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "Y \\sim \\mathcal{N}(X\\beta, \\sigma^2 I),\n",
    "\\end{equation}\n",
    "\n",
    "where $I$ is the $m \\times m$ identity matrix. Also, $\\hat{\\beta}$ is a linear transformation of $Y$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}=AY,\n",
    "\\end{equation}\n",
    "\n",
    "where $A$ is the following $(n+1)\\times m$ dimensional matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "A \\equiv (X^TX)^{-1}X^T.\n",
    "\\end{equation}\n",
    "\n",
    "Because of this, $\\hat{\\beta}$ itself has a multivariate Gaussian distribution. This is a general result which we won't prove here. Instead, let's compute the mean and covariance of $\\hat{\\beta}$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\hat{\\beta}_j \\rangle\n",
    "    = \\sum_{i=1}^{m}A_{ji}\\langle y^{(i)} \\rangle\n",
    "    = \\sum_{i=1}^{m}A_{ji}\\beta^Tx^{(i)}\n",
    "    = \\sum_{i=1}^{m}A_{ji}[X\\beta]_i\n",
    "    = [AX\\beta]_j\n",
    "    = [(X^TX)^{-1}X^TX\\beta]_j\n",
    "    =\\beta_j.\n",
    "\\end{equation}\n",
    "\n",
    "Good! Our point estimates are unbiased. Let's move on the covariance. Note that because of the first equality above:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}_j - \\beta_j = \\sum_{i=1}^{m}A_{ji} (y^{(i)} - \\langle y^{(i)} \\rangle),\n",
    "\\end{equation}\n",
    "\n",
    "We can now use this to compute the covariance:\n",
    "\n",
    "\\begin{align*}\n",
    "\\langle (\\hat{\\beta}_j - \\beta_j)(\\hat{\\beta}_{j'} - \\beta_{j'}) \\rangle\n",
    "    &= \\sum_{i,i'=1}^{m}A_{ji}A_{j'i'}\\left\\langle(y^{(i)} - \\langle y^{(i)} \\rangle)(y^{(i')} - \\langle y^{(i')} \\rangle)\\right\\rangle \\\\\n",
    "    &= \\sum_{i,i'=1}^{m}A_{ji}A_{j'i'}\\sigma^2\\delta_{ii'} \\\\\n",
    "    &= \\sigma^2\\sum_{i}^{m}A_{ji}A_{j'i} \\\\\n",
    "    &= \\sigma^2[AA^T]_{jj'} \\\\\n",
    "    &= \\sigma^2[(X^TX)^{-1}X^TX(X^TX)^{-1}]_{jj'} \\\\\n",
    "    &= \\sigma^2[(X^TX)^{-1}]_{jj'}.\n",
    "\\end{align*}\n",
    "\n",
    "What an elegant expression! For simple linear regression we had to do a lot more work to compute the the variance of $\\hat{\\alpha}$, $\\hat{\\beta}$, and also their covariance. Above we computed $(X^TX)^{-1}$ for univariate linear regression. Using that, it's easy to check that the formulas derived [here](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/linreg.ipynb) are consistent with the covariance matrix $\\sigma^2[(X^TX)^{-1}]$.\n",
    "\n",
    "To summarize, $\\hat{\\beta}$ has a multivariate Gaussian distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} \\sim \\mathcal{N}(\\beta, \\sigma^2(X^TX)^{-1}).\n",
    "\\end{equation}\n",
    "\n",
    "Let's check this against `statsmodels`: (we will use `statsmodels` estimate of $\\sigma^2$ by using the `scale` attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Beta_0-Beta_0 covariance = 0.0202562993414\n",
      "Reproduced  Beta_0-Beta_0 covariance = 0.0202562993414\n",
      "\n",
      "statsmodels Beta_0-Beta_1 covariance = -0.0202764718329\n",
      "Reproduced  Beta_0-Beta_1 covariance = -0.0202764718329\n",
      "\n",
      "statsmodels Beta_0-Beta_2 covariance = -0.0155511241659\n",
      "Reproduced  Beta_0-Beta_2 covariance = -0.0155511241659\n",
      "\n",
      "statsmodels Beta_1-Beta_1 covariance = 0.0374243151515\n",
      "Reproduced  Beta_1-Beta_1 covariance = 0.0374243151515\n",
      "\n",
      "statsmodels Beta_1-Beta_2 covariance = 0.00299109336668\n",
      "Reproduced  Beta_1-Beta_2 covariance = 0.00299109336668\n",
      "\n",
      "statsmodels Beta_2-Beta_2 covariance = 0.0278181525471\n",
      "Reproduced  Beta_2-Beta_2 covariance = 0.0278181525471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beta_cov = model.scale * np.linalg.inv(X.T.dot(X))\n",
    "statsmodels_cov = model.cov_params()\n",
    "\n",
    "for i in range(len(beta)):\n",
    "    for j in range(i, len(beta)):\n",
    "        print(\"statsmodels Beta_{0}-Beta_{1} covariance = {2}\".format(i, j, statsmodels_cov[i, j]))\n",
    "        print(\"Reproduced  Beta_{0}-Beta_{1} covariance = {2}\".format(i, j, beta_cov[i, j]))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the unbiased esimate of $\\sigma^2$? Note that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{e}\n",
    "    = Y - X\\hat{\\beta}\n",
    "    = Y - X(X^TX)^{-1}X^TY\n",
    "    = (I - X(X^TX)^{-1}X^T)Y,\n",
    "\\end{equation}\n",
    "\n",
    "where $I$ is the $m \\times m$ dimensional identity matrix. Let \n",
    "\n",
    "\\begin{equation}\n",
    "M \\equiv I - X(X^TX)^{-1}X^T,\n",
    "\\end{equation}\n",
    "\n",
    "so that \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{e} = MY.\n",
    "\\end{equation}\n",
    "\n",
    "It's easy to check that $M$ is symmetric and also equal to its own square: $M^2=M$. Let $\\langle Y \\rangle$ denote\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle Y \\rangle\n",
    "    \\equiv\n",
    "    \\begin{pmatrix}\n",
    "        \\langle y^{(1)} \\rangle \\\\\n",
    "        \\langle y^{(2)} \\rangle \\\\\n",
    "        \\vdots \\\\\n",
    "        \\langle y^{(m)} \\rangle\n",
    "    \\end{pmatrix} =\n",
    "    \\begin{pmatrix}\n",
    "        \\beta^Tx^{(1)} \\\\\n",
    "        \\beta^Tx^{(2)} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\beta^Tx^{(m)}\n",
    "    \\end{pmatrix}\n",
    "    = X\\beta.\n",
    "\\end{equation}\n",
    "\n",
    "Then:\n",
    "\n",
    "\\begin{equation}\n",
    "M\\langle Y \\rangle = (I - X(X^TX)^{-1}X^T)X\\beta = X\\beta - X\\beta = 0,\n",
    "\\end{equation}\n",
    "\n",
    "so we can also write $\\hat{e}$ as\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{e} = M(Y - \\langle Y \\rangle).\n",
    "\\end{equation}\n",
    "\n",
    "The sum of squared residuals is given by the norm of $\\hat{e}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{e}^T\\hat{e}\n",
    "    &= (Y - \\langle Y \\rangle)^TM^TM(Y - \\langle Y \\rangle) \\\\\n",
    "    &= (Y - \\langle Y \\rangle)^TM(Y - \\langle Y \\rangle).\n",
    "\\end{align*}\n",
    "\n",
    "Taking the expectation value we find\n",
    "\n",
    "\\begin{align*}\n",
    "\\langle\\hat{e}^T\\hat{e}\\rangle\n",
    "    &= \\left\\langle(Y - \\langle Y \\rangle)^TM(Y - \\langle Y \\rangle)\\right\\rangle \\\\\n",
    "    &= \\sum_{ii'}M_{ii'}\\left\\langle(y^{(i)} - \\langle y^{(i)} \\rangle)(y^{(i')} - \\langle y^{(i')} \\rangle)\\right\\rangle \\\\\n",
    "    &= \\sum_{ii'}M_{ii'}\\sigma^2\\delta_{ii'} \\\\\n",
    "    &= \\sigma^2 \\text{tr}(M),\n",
    "\\end{align*}\n",
    "\n",
    "where the third equality follow from $\\left\\langle(y^{(i)} - \\langle y^{(i)} \\rangle)(y^{(i')} - \\langle y^{(i')} \\rangle)\\right\\rangle = \\sigma^2 \\delta_{ii'}$. We had arrived at the exact same formula for simple linear regression as well. Next, we need to find the trace of $M$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{tr}(M)\n",
    "    = \\text{tr}(I) - \\text{tr}(X(X^TX)^{-1}X^T)\n",
    "    = m - \\text{tr}(X^TX(X^TX)^{-1})\n",
    "    = m - (n+1),\n",
    "\\end{equation}\n",
    "\n",
    "where the second equality uses the linear algebra identity $\\text{tr}(AB)=\\text{tr}(BA)$. Therefore, the unbiased estimator $\\hat{\\sigma}^2$ of $\\sigma^2$ is\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\sigma}^2 = \\frac{1}{m - n - 1}\\hat{e}^T\\hat{e}.\n",
    "\\end{equation}\n",
    "\n",
    "For simple linear regression $n=1$ and we recover the $m-2$ factor in the denominator.\n",
    "\n",
    "Let's see if this reproduces the estimate of `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels estimate of sigma^2 = 0.223401966576\n",
      "Reproduced  estimate of sigma^2 = 0.223401966576\n"
     ]
    }
   ],
   "source": [
    "yvar = np.dot(e, e)/(m-len(beta))\n",
    "print(\"statsmodels estimate of sigma^2 = {}\".format(model.scale))\n",
    "print(\"Reproduced  estimate of sigma^2 = {}\".format(yvar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence intervals\n",
    "\n",
    "Let's work our confidence intervals for $\\hat{\\beta}_j$, so we can make statements like: there's a $95\\%$ chance that the interval $(\\beta^{(L)}_j, \\beta^{(U)}_j)$ contains $\\beta_j$. We've already shown that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}_j \\sim \\mathcal{N}(\\beta_j, \\sigma^2\\Sigma_{jj}),\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "\\Sigma \\equiv (X^TX)^{-1}.\n",
    "\\end{equation}\n",
    "\n",
    "This means $\\beta^{(L)}_j=\\hat{\\beta}_j-z\\sqrt{\\sigma^2\\Sigma_{jj}}$ and $\\beta^{(U)}_j=\\hat{\\beta}_j+z\\sqrt{\\sigma^2\\Sigma_{jj}}$ define a valid confidence interval for confidence level $1 - \\gamma$, where $z=\\Phi^{-1}(1-\\gamma/2)$ and $\\Phi_t^{-1}$ is the inverse CDF of the standard normal distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "P\\left(\\hat{\\beta}_j-z \\sqrt{\\sigma^2\\Sigma_{jj}} \\le \\beta_j \\le \\hat{\\beta}_j+z\\sqrt{\\sigma^2\\Sigma_{jj}}\\right)\n",
    "= P\\left(-z \\le \\frac{\\hat{\\beta}_j - \\beta_j}{\\sqrt{\\sigma^2\\Sigma_{jj}}} \\le z \\right)\n",
    "= 2\\Phi(z)-1\n",
    "= 2\\Phi(\\Phi^{-1}(1-\\gamma/2))-1\n",
    "= 1-\\gamma.\n",
    "\\end{equation}\n",
    "\n",
    "In other words, there's a $1-\\gamma$ chance that the interval $(\\beta^{(L)}_j, \\beta^{(U)}_j)$ contains $\\beta_j$. The problem, though, is that we do not know $\\sigma^2$. What if we replace it with the estimate $\\hat{\\sigma}^2$? Then we'll need to know the distribution of\n",
    "\n",
    "\\begin{equation}\n",
    "T \\equiv\n",
    "    \\frac{\\hat{\\beta}_j - \\beta_j}{\\sqrt{\\hat{\\sigma}^2\\Sigma_{jj}}}\n",
    "    = \\frac{(\\hat{\\beta}_j - \\beta_j)/\\sqrt{\\sigma^2\\Sigma_{jj}}}{\\sqrt{\\frac{1}{(m - n - 1)\\sigma^2}\\hat{e}^T\\hat{e}}}\n",
    "    = \\frac{Z}{\\sqrt{\\frac{V}{m-n-1}}},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "Z \\equiv \\frac{\\hat{\\beta}_j - \\beta_j}{\\sqrt{\\sigma^2\\Sigma_{jj}}}, \\qquad\n",
    "V \\equiv \\frac{1}{\\sigma^2}\\hat{e}^T\\hat{e}.\n",
    "\\end{equation}\n",
    "\n",
    "We already know that $Z \\sim \\mathcal{N}(0, 1)$. It can also be shown that $V$ has a chi-squared distribution with $m-n-1$ degrees of freedom and that $Z$ and $V$ are independent. Therefore, $T$ has a t-distribution with $m-n-1$ degrees of freedom. (This is not trivial to show. We presented a proof for simple linear regression using Cochran's theorem. For some reason I haven't bothered working it out for multiple linear regression. I suppose every now and then I choose not to obsess.)\n",
    "\n",
    "This means that $\\beta^{(L)}_j=\\hat{\\beta}_j-t\\sqrt{\\hat{\\sigma}^2\\Sigma_{jj}}$ and $\\beta^{(U)}_j=\\hat{\\beta}_j+t\\sqrt{\\hat{\\sigma}^2\\Sigma_{jj}}$ define a valid confidence interval for confidence level $1-\\gamma$ ($0\\le \\gamma \\le 1$), where $t=\\Phi_t^{-1}(1-\\gamma/2)$ and $\\Phi_t^{-1}$ is the inverse CDF of the t-distribution with $m-n-1$ degrees of freedom. This gives us the same confidence interval as `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Beta_0 95% confidence interval = (0.102598895008, 0.667548867756)\n",
      "Reproduced  Beta_0 95% confidence interval = (0.102598895008, 0.667548867756)\n",
      "\n",
      "statsmodels Beta_1 95% confidence interval = (1.38621407759, 2.15411797136)\n",
      "Reproduced  Beta_1 95% confidence interval = (1.38621407759, 2.15411797136)\n",
      "\n",
      "statsmodels Beta_2 95% confidence interval = (4.66383629856, 5.32589131574)\n",
      "Reproduced  Beta_2 95% confidence interval = (4.66383629856, 5.32589131574)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# statsmodel confidence interval\n",
    "statsmodels_ci = model.conf_int()\n",
    "t = stats.t.ppf(1 - 0.05/2., m - len(beta))\n",
    "\n",
    "for i in range(len(beta)):\n",
    "    beta_se = np.sqrt(beta_cov[i, i])\n",
    "    print(\"statsmodels Beta_{0} 95% confidence interval = ({1}, {2})\".format(i, statsmodels_ci[i, 0], statsmodels_ci[i, 1]))\n",
    "    print(\"Reproduced  Beta_{0} 95% confidence interval = ({1}, {2})\".format(i, beta[i] - t*beta_se, beta[i] + t*beta_se))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For unseen values of the independent variables $x$, we predict $y$ using $\\hat{f}(x) \\equiv \\hat{\\beta}^Tx$, which is subject to statistical noise. The mean is what we want:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\hat{f}(x) \\rangle = \\beta^Tx.\n",
    "\\end{equation}\n",
    "\n",
    "The variance is then given by\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\langle \\left(\\hat{f}(x) - \\langle \\hat{f}(x) \\rangle\\right)^2 \\right\\rangle\n",
    "    &= \\sum_{j,j'=0}^{n} x_jx_{j'}\\langle (\\hat{\\beta}_j - \\beta_j)(\\hat{\\beta}_{j'} - \\beta_{j'}) \\rangle \\\\\n",
    "    &= \\sum_{j,j'=0}^{n} x_jx_{j'}\\sigma^2[(X^TX)^{-1}]_{jj'} \\\\\n",
    "    &= \\sigma^2 x^T (X^TX)^{-1}x.\n",
    "\\end{align*}\n",
    "\n",
    "Does this match the expression we derived for simple linear regression? Let's check:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma^2 x^T (X^TX)^{-1}x\n",
    "    &= \\frac{\\sigma^2}{m S_x^2}\n",
    "    \\begin{pmatrix} 1 \\\\ x \\end{pmatrix}^T\n",
    "    \\begin{pmatrix} S_x^2 + \\bar{x}^2 && -\\bar{x} \\\\ -\\bar{x} && 1 \\end{pmatrix}\n",
    "    \\begin{pmatrix} 1 \\\\ x \\end{pmatrix} \\\\\n",
    "    &= \\frac{\\sigma^2}{m S_x^2} (S_x^2 + \\bar{x}^2 - 2x\\bar{x} +x^2) \\\\\n",
    "    &= \\frac{\\sigma^2}{m S_x^2} (S_x^2 + (x - \\bar{x})^2) \\\\\n",
    "    &= \\frac{\\sigma^2}{m} \\left[1 + \\frac{(x - \\bar{x})^2}{S_x^2}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "Indeed! To get confidence intervals for our predictions, we will proceed as before: $f_L=\\hat{f}-ts_{\\hat{f}}$ and $f_U=\\hat{f}+ts_{\\hat{f}}$ define a valid confidence interval for confidence level $1-\\gamma$ ($0\\le \\gamma \\le 1$), where $t=\\Phi_t^{-1}(1-\\gamma/2)$, $\\Phi_t^{-1}$ is the inverse CDF of the t-distribution with $m-n-1$ degrees of freedom, and\n",
    "\n",
    "\\begin{equation}\n",
    "s_{\\hat{f}}^2 = \\hat{\\sigma}^2 x^T (X^TX)^{-1}x.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When the normal equation fails\n",
    "\n",
    "Can $X^TX$ ever be non-invertible? Yes! Below we will cover two such scenarios.\n",
    "\n",
    "If one of the features is a linear combination of the other ones, $X^TX$ will not be invertible. For example, suppose we include two features in our data set, but they are related via $x_2 = 2x_1 + 3$. Then the determinant of $X^TX$ will be zero, which means it's not invertible: (note that because of numerical noise, the determinant will not be exactly zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1872293550114626e-10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = np.column_stack([np.ones(m), x1, 2*x1+3])\n",
    "np.linalg.det(X2.T.dot(X2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does `statsmodels` then fail to produce the estimates $\\hat{\\beta}$? Let's check by generating data using\n",
    "\n",
    "\\begin{equation}\n",
    "y(x_1, x_2) \\sim \\mathcal{N}(0.3 + x_1 + x_2, 0.5^2)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.28363415,  0.98799979,  1.12509713])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = np.random.normal(loc=X2.dot(np.array([0.3, 1, 1])), scale=0.5, size=m)\n",
    "model2 = regression.linear_model.OLS(y2, X2).fit()\n",
    "model2.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we're still getting numbers from `statsmodels`. What's going on? Instead of using the actual inverse of $X^TX$, `statsmodels` is using the *pseudo*-inverse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.28363415,  0.98799979,  1.12509713])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X2.T.dot(X2)).dot(X2.T).dot(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudo-inverse of a symmetric matrix $A$ is the inverse confined to the subspace spanned by the eigenvectors of $A$ which have corresponding non-zero eigenvalues. Let's rephrase that in a less obnoxious way. Consider the diagonalization of $A=UDU^T$ where $U$'s columns are orthonomal eigenvectors of $A$, i.e. $U^T=U=U^T=I$, and $D$ is a diagonal matrix whose diagonal elements are the eigenvalues of $A$. The inverse of $A$ is given by: $A^{-1}=UD^{-1}U^T$. Taking the inverse of $D$ just involves taking the numerical inverse of its diagonal elements. If one of these is zero, though, $A$ cannot be inverted. The pseudo-inverse only inverts the non-zero diagonal elements of $D$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo-inverse using numpy.linalg.pinv\n",
      "--------------------------------------\n",
      "[[ 0.06259946 -0.09530321 -0.00280806]\n",
      " [-0.09530321  0.14522771  0.00454578]\n",
      " [-0.00280806  0.00454578  0.0006674 ]]\n",
      "\n",
      "Reproduced pseudo-inverse\n",
      "--------------------------------------\n",
      "[[ 0.06259946 -0.09530321 -0.00280806]\n",
      " [-0.09530321  0.14522771  0.00454578]\n",
      " [-0.00280806  0.00454578  0.0006674 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pseudo-inverse using numpy.linalg.pinv\")\n",
    "print(\"--------------------------------------\")\n",
    "print(np.linalg.pinv(X2.T.dot(X2)))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reproduced pseudo-inverse\")\n",
    "print(\"--------------------------------------\")\n",
    "eigvals, eigvecs = np.linalg.eig(X2.T.dot(X2))\n",
    "dinv = np.diag([1/eigval if abs(eigval) > 1e-7 else 0 for eigval in eigvals])\n",
    "print(eigvecs.dot(dinv).dot(eigvecs.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $x_2$ and $x_1$ are related, we can rewrite the model as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_1, x_2)\n",
    "    = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2\n",
    "    = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2(2x_1 + 3)\n",
    "    = (\\hat{\\beta}_0 + 3\\hat{\\beta}_2) + (\\hat{\\beta}_1 + 2\\hat{\\beta}_2)x_1.\n",
    "\\end{equation}\n",
    "\n",
    "This suggests that the following new estimates should give us the exact same prediction:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\beta}_0 = \\hat{\\beta}_0 + 3\\hat{\\beta}_2, \\qquad\n",
    "\\tilde{\\beta}_1 = \\hat{\\beta}_1 + 2\\hat{\\beta}_2, \\qquad\n",
    "\\tilde{\\beta}_2 = 0.\n",
    "\\end{equation}\n",
    "\n",
    "Let's check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction with beta = [-0.28363415  0.98799979  1.12509713]: 4.38693484364\n",
      "Prediction with beta = [3.09165723 3.23819404 0.        ]: 4.38693484364\n"
     ]
    }
   ],
   "source": [
    "beta_tilde = np.array([\n",
    "    model2.params[0] + 3*model2.params[2],\n",
    "    model2.params[1] + 2*model2.params[2],\n",
    "    0\n",
    "])\n",
    "\n",
    "x_new = np.array([1, 0.4, 2*0.4 + 3])\n",
    "print(\"Prediction with beta = {0}: {1}\".format(model2.params, model2.params.dot(x_new)))\n",
    "print(\"Prediction with beta = {0}: {1}\".format(beta_tilde, beta_tilde.dot(x_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, there are infinitely many $\\hat{\\beta}$s that will produce the exact same prediction. This can be quite dangerous!\n",
    "\n",
    "Another way $X^TX$ can become non-invertible is if the number of observations is less than the nmber of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.1390979942215958e-17"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3 = np.column_stack([np.ones(2), x1[:2], x2[:2]])\n",
    "np.linalg.det(X3.T.dot(X3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-test\n",
    "\n",
    "Consider the following null hypothesis: $y$ does not depend on any of the explanatory variables $x_1, \\dots, x_n$ and the correct model of the data is simply $y=\\alpha$, where $\\alpha$ is some constant. If we cannot reject this null hypothesis at a reasonable confidence level, we should probably be concerned. In such a scenario, we will likely also see a low $R^2$, since $y=const$ is able to explain the data better than our model.\n",
    "\n",
    "As it turns out, the following test-statistic that has the [$F(n,m-n-1)$](https://en.wikipedia.org/wiki/F-distribution) distribution under the null hypothesis $y^{(i)} \\sim \\mathcal{N}(\\alpha, \\sigma^2)$:\n",
    "\n",
    "\\begin{equation}\n",
    "f_{stat} = \\frac{m - n - 1}{n}\\frac{\\sum_{i=1}^{m}(y^{(i)} - \\bar{y})^2 - \\hat{e}^T\\hat{e}}{\\hat{e}^T\\hat{e}}.\n",
    "\\end{equation}\n",
    "\n",
    "Let's reproduce the F-statistic from `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels F-statistic = 468.915176037\n",
      "Reproduced  F-statistic = 468.915176037\n"
     ]
    }
   ],
   "source": [
    "ftest = (np.dot(y - ymean, y - ymean) - np.dot(e, e))/np.dot(e, e) * (m - p) / (p - 1)\n",
    "\n",
    "print(\"statsmodels F-statistic = {}\".format(model.fvalue))\n",
    "print(\"Reproduced  F-statistic = {}\".format(ftest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can empirically check that the distribution of the test-statistic above is indeed what we're claiming. To do so, we will do the following:\n",
    "1. Keep the design matrix fixed, i.e do not change the values of $x_1, \\dots, x_N$.\n",
    "2. Generate $m$ samples from $\\mathcal{N}(\\alpha, \\sigma^2)$.\n",
    "3. Solve for the estimates $\\hat{\\beta}$, as though $y$ depends on $x_1, \\dots, x_N$ (while it actually doesn't).\n",
    "4. Compute the residual vector $\\hat{e}=MY$, where as before $M=I - X(X^TX)^{-1}X^T$.\n",
    "5. Compute $f_{stat}$.\n",
    "6. Repeat above steps many times to get a distribution for $f_{stat}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1125bf610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n4 = 5\n",
    "X4 = np.column_stack((np.ones(m), np.random.uniform(size=(m, n4))))\n",
    "M = np.identity(m) - X4.dot(np.linalg.inv(X4.T.dot(X4))).dot(X4.T)\n",
    "\n",
    "fvals = []\n",
    "for _ in range(50000):\n",
    "    y_f = np.random.normal(loc=6, scale=0.5, size=100)\n",
    "    e_f = M.dot(y_f)\n",
    "    # Total sum of squares\n",
    "    tss = m * np.var(y_f)\n",
    "    # Residual sum of squares\n",
    "    rss = np.dot(e_f, e_f)\n",
    "    fvals.append(((tss - rss) / n4) / (rss / (m - n4 - 1)))\n",
    "\n",
    "plt.hist(fvals, bins=100, normed=True);\n",
    "fx = np.linspace(0.01, 6, 100)\n",
    "plt.plot(fx, stats.f.pdf(fx, n4, m - n4 - 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We can also prove this analytically. Let $1_m$ denote the $m$-dimensional vector of $1$s. Then, we can express the mean of $Y$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle Y \\rangle = \\alpha 1_m.\n",
    "\\end{equation}\n",
    "\n",
    "Note that\n",
    "\n",
    "\\begin{equation}\n",
    "M\\langle Y \\rangle\n",
    "    = M(\\alpha 1_m)\n",
    "    = \\alpha M 1_m\n",
    "    = \\alpha(I - X(X^TX)^{-1}X^T)1_m\n",
    "    = \\alpha(1_m - 1_m)=0,\n",
    "\\end{equation}\n",
    "\n",
    "where the second last equality follows from $X(X^TX)^{-1}X^T1_m = 1_m$ (to see why, apply $X^T$ to both sides). As a result, we can express $\\hat{e}$ follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{e} = M(Y - \\langle Y \\rangle).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{e}^T\\hat{e}\n",
    "    = (Y - \\langle Y \\rangle)^TM^TM(Y - \\langle Y \\rangle)\n",
    "    = (Y - \\langle Y \\rangle)^TM(Y - \\langle Y \\rangle),\n",
    "\\end{equation}\n",
    "\n",
    "where the last equality follows from $M^T=M$ and $M^2=M$. Also note that\n",
    "\n",
    "\\begin{equation}\n",
    "y^{(i)} - \\bar{y}\n",
    "    = \\sum_{k=1}^{m} (\\delta_{ik} - 1/m)y^{(k)}\n",
    "    = \\sum_{k=1}^{m} (\\delta_{ik} - 1/m)(y^{(k)} - \\alpha)\n",
    "    = \\sum_{k=1}^{m} W_{ik}(y^{(k)} - \\langle y^{(k)} \\rangle),\n",
    "\\end{equation}\n",
    "\n",
    "where the second equality follows from $\\sum_{k=1}^{m} (\\delta_{ik} - 1/m)=0$ and we've introduced the matrix $W$:\n",
    "\n",
    "\\begin{equation}\n",
    "W_{ik} \\equiv \\delta_{ik} - \\frac{1}{m}.\n",
    "\\end{equation}\n",
    "\n",
    "It's easy to check that, just like $M$, $W$ is idempotent (i.e. $W^2=W$). This can in turn be used to show:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{m}(y^{(i)} - \\bar{y})^2 = (Y - \\langle Y \\rangle)^TW(Y - \\langle Y \\rangle).\n",
    "\\end{equation}\n",
    "\n",
    "The numerator of our test-statistic can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{m}(y^{(i)} - \\bar{y})^2 - \\hat{e}^T\\hat{e} = (Y - \\langle Y \\rangle)^T(W - M)(Y - \\langle Y \\rangle).\n",
    "\\end{equation}\n",
    "\n",
    "Note that $W - M$ is also idempotent:\n",
    "\n",
    "\\begin{equation}\n",
    "(W - M)^2\n",
    "    = W^2 - WM - WB + M^2\n",
    "    = W - M - M + M\n",
    "    = W - M,\n",
    "\\end{equation}\n",
    "\n",
    "where the second equality uses $WM=MW=M$. Consider now the following definitions:\n",
    "\n",
    "\\begin{equation}\n",
    "B_1 \\equiv M, \\qquad\n",
    "B_2 \\equiv W - M, \\qquad\n",
    "B_3 \\equiv I - W,\n",
    "\\end{equation}\n",
    "\n",
    "as well as\n",
    "\n",
    "\\begin{equation}\n",
    "Q_1 \\equiv (Y - \\langle Y \\rangle)^TB_1(Y - \\langle Y \\rangle), \\qquad\n",
    "Q_2 \\equiv (Y - \\langle Y \\rangle)^TB_2(Y - \\langle Y \\rangle), \\qquad\n",
    "Q_3 \\equiv (Y - \\langle Y \\rangle)^TB_3(Y - \\langle Y \\rangle).\n",
    "\\end{equation}\n",
    "\n",
    "$B_1$, $B_2$, and $B_3$ are idempotent, so their ranks are given by their traces. It then follows from  $\\text{tr}(M) = m - n - 1$ (we have proven this already) and $\\text{tr}(W) = m - 1$ that: $\\text{rank}(B_1) = m - n - 1$, $\\text{rank}(B_2) = n$, $\\text{rank}(B_3) = 1$.\n",
    "\n",
    "Since $B_1 + B_2 + B_3 = I$ and $\\text{rank}(B_1)+\\text{rank}(B_1)+\\text{rank}(B_1)=m$, the conditions of Cochran's theorem are satisfied. It then follows that $Q_1 \\sim \\chi^2_{m-n-1}$, $Q_2 \\sim \\chi^2_{n}$, and that $Q_1$ and $Q_2$ are independent. This in turn implies that\n",
    "\n",
    "\\begin{equation}\n",
    "f = \\frac{m - n - 1}{n}\\frac{\\sum_{i=1}^{m}(y^{(i)} - \\bar{y})^2 - \\hat{e}^T\\hat{e}}{\\hat{e}^T\\hat{e}}\n",
    "  = \\frac{Q_2/n}{Q_1/(m-n-1)}\n",
    "\\end{equation}\n",
    "\n",
    "is distributed as $F(n,m-n-1)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
