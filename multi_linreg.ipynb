{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with multiple variables\n",
    "\n",
    "The goal of this notebook is to generalize [linear regression with one variable](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/linreg.ipynb) (also called *simple* linear regression) to multiple variables. The model is a natural extension to $n$ independent variables (or *features*):\n",
    "\n",
    "\\begin{equation}\n",
    "f(x_1, x_2, \\dots, x_n) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n.\n",
    "\\end{equation}\n",
    "\n",
    "A single observation corresponds to a value for each $x_1, x_2, \\dots, x_n$, as well as the dependent variable $y$. We will assume there are $m$ such observations. Let's start by establishing notation:\n",
    "\n",
    "* $y^{(i)}$: value of the dependent variable for the $i^{\\text{th}}$ observation.\n",
    "* $x^{(i)}_{j}$: value of the $j^{\\text{th}}$ feature for the $i^{\\text{th}}$ observation.\n",
    "* $x_0 \\equiv 1$. In other words, $x^{(i)}_{0} = 1$ for all $i=1,\\dots,m$.\n",
    "* $x=\\begin{pmatrix}x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_n\\end{pmatrix} \\in \\mathbb{R}^{n+1}$ denotes the vector of features.\n",
    "* $x^{(i)}=\\begin{pmatrix}x^{(i)}_0 \\\\ x^{(i)}_1 \\\\ \\vdots \\\\ x^{(i)}_n\\end{pmatrix} \\in \\mathbb{R}^{n+1}$ denotes the vector of features for the $i^{\\text{th}}$ observation.\n",
    "* $\\beta=\\begin{pmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_n\\end{pmatrix} \\in \\mathbb{R}^{n+1}$ denotes the vector of parameters.\n",
    "* $X_{ij} \\equiv x^{(i)}_{j}$ for $i=1,\\dots,m$ and $j=0,\\dots,n$. $X$ is called the *design matrix*. It's an $m \\times (n+1)$ dimensional matrix where each row contains the value of features for a given observation. Note that the first column is just a vector of $1$s, since by definition $x^{(i)}_{0} = 1$.\n",
    "* $Y_i \\equiv y^{(i)}$: $m$-dimensional vector created from $y^{(1)}, \\dots, y^{(m)}$. It's sometimes called the *target vector*.\n",
    "\n",
    "Expressing the data in vectorized form will make our life much easier. For instance, $f$ can now be expressed as a dot product:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\beta^Tx.\n",
    "\\end{equation}\n",
    "\n",
    "We will use the same cost function as we did for simple linear regression, i.e. the normalized sum of squared prediction errors:\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\beta) &= \\frac{1}{2m}\\sum_{i=1}^{m} (f(x^{(i)}) - y^{(i)})^2 \\\\\n",
    "         &= \\frac{1}{2m}\\sum_{i=1}^{m} (\\beta^Tx^{(i)} - y^{(i)})^2.\n",
    "\\end{align*}\n",
    "\n",
    "Note that\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta^Tx^{(i)} = \\sum_{j=0}^{n} \\beta_j x^{(i)}_{j} = \\sum_{j=0}^{n} X_{ij}\\beta_j = [X\\beta]_{i}.\n",
    "\\end{equation}\n",
    "\n",
    "This allows us to express $J$ in matrix form:\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\beta) &= \\frac{1}{2m}\\sum_{i=1}^{m} (\\beta^Tx^{(i)} - y^{(i)})^2 \\\\\n",
    "         &= \\frac{1}{2m}\\sum_{i=1}^{m} ([X\\beta]_{i} - Y_i)^2 \\\\\n",
    "         &= \\frac{1}{2m}\\sum_{i=1}^{m} [X\\beta - Y]_{i}[X\\beta - Y]_{i} \\\\\n",
    "         &= \\frac{1}{2m}[X\\beta - Y]^T[X\\beta - Y].\n",
    "\\end{align*}\n",
    "\n",
    "To find the parameters $\\beta_0,\\beta_1, \\dots, \\beta_n$ that best fit the data set, we will minimize $J$ with respect to each one of them. To do that, first we need the gradient of $J$ with respect to each $\\beta_j$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial \\beta_j}\n",
    "    &= \\frac{1}{m}\\sum_{i=1}^{m}(\\beta^Tx^{(i)} - y^{(i)})x^{(i)}_{j} \\\\\n",
    "    &= \\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[X\\theta - Y]_{i} \\\\\n",
    "    &= \\frac{1}{m} [X^T(X\\beta - Y)]_j.\n",
    "\\end{align*}\n",
    "\n",
    "Setting the gradients to zero:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla J = 0 \\to X^T(X\\hat{\\beta} - Y) = 0,\n",
    "\\end{equation}\n",
    "\n",
    "we find that the optimal parameters $\\hat{\\beta}$ are given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} = (X^TX)^{-1}X^TY.\n",
    "\\end{equation}\n",
    "\n",
    "This is called the *normal equation*. Does it reproduce the formulas we found for $\\hat{\\alpha}$ and $\\hat{\\beta}$ in [simple linear regression](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/linreg.ipynb)? Let's check. First note the mapping between the variables:\n",
    "\n",
    "| Multiple LR     | Simple LR       |\n",
    "|-----------------|-----------------|\n",
    "| $x^{(i)}_1$     | $x^{(i)}$       |\n",
    "| $\\hat{\\beta}_0$ | $\\hat{\\alpha}$  |\n",
    "| $\\hat{\\beta}_1$ | $\\hat{\\beta}$   |\n",
    "\n",
    "Using this, it can be shown after straightforward algebra that (see [this](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/linreg.ipynb) notebook for undefined quantities below)\n",
    "\n",
    "\\begin{equation}\n",
    "X^TX = m\n",
    "\\begin{pmatrix}\n",
    "    1       & \\bar{x} \\\\\n",
    "    \\bar{x} & S_x^2 + \\bar{x}^2\n",
    "\\end{pmatrix}, \\qquad\n",
    "X^TY = m\n",
    "\\begin{pmatrix}\n",
    "    \\bar{y} \\\\\n",
    "    C_{xy} + \\bar{x}\\bar{y}\n",
    "\\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "The inverse of $X^TX$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "(X^TX)^{-1} = \\frac{1}{mS_x^2}\n",
    "\\begin{pmatrix}\n",
    "    S_x^2 + \\bar{x}^2       & -\\bar{x} \\\\\n",
    "    -\\bar{x} & 1\n",
    "\\end{pmatrix},\n",
    "\\end{equation}\n",
    "\n",
    "using which we can compute $\\hat{\\beta}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}\n",
    "    = (X^TX)^{-1}X^TY\n",
    "    = \\frac{1}{S_x^2}\n",
    "        \\begin{pmatrix} S_x^2 + \\bar{x}^2 & -\\bar{x} \\\\ -\\bar{x} & 1 \\end{pmatrix}\n",
    "        \\begin{pmatrix} \\bar{y} \\\\ C_{xy} + \\bar{x}\\bar{y} \\end{pmatrix}\n",
    "    = \\frac{1}{S_x^2}\n",
    "    \\begin{pmatrix} \\bar{y}S_x^2 -\\bar{x}C_{xy} \\\\ C_{xy} \\end{pmatrix}\n",
    "    = \\begin{pmatrix} \\bar{y} -\\bar{x}\\frac{C_{xy}}{S_x^2} \\\\ \\frac{C_{xy}}{S_x^2} \\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "That's exactly what we had previously derived for simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression with `statsmodels`\n",
    "\n",
    "Below we will create a fake dataset, run linear regression on it using `statsmodels`, and try to reproduce the various outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels import regression\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate data using the model\n",
    "\n",
    "\\begin{equation}\n",
    "y(x_1, x_2) \\sim 0.3 + 2x_1 + 5 x_2 + \\epsilon, \\qquad\n",
    "\\epsilon \\sim \\mathcal{N}(0, 0.5^2).\n",
    "\\end{equation}\n",
    "\n",
    "So, the \"true\" values of the parameters we will be estimating are $\\beta_0 = 0.3$, $\\beta_1 = 2$, and $\\beta_2 = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make results reproducible\n",
    "np.random.seed(123)\n",
    "\n",
    "m = 100\n",
    "x1 = np.random.uniform(size=m)\n",
    "x2 = np.random.uniform(size=m)\n",
    "y = np.random.normal(loc=0.3 + 2.0*x1 + 5.0*x2, scale=0.5, size=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the data using `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.906</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.904</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   468.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 27 Mar 2019</td> <th>  Prob (F-statistic):</th> <td>1.37e-50</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:10:16</td>     <th>  Log-Likelihood:    </th> <td> -65.432</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   136.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    97</td>      <th>  BIC:               </th> <td>   144.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.3851</td> <td>    0.142</td> <td>    2.706</td> <td> 0.008</td> <td>    0.103</td> <td>    0.668</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    1.7702</td> <td>    0.193</td> <td>    9.150</td> <td> 0.000</td> <td>    1.386</td> <td>    2.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    4.9949</td> <td>    0.167</td> <td>   29.947</td> <td> 0.000</td> <td>    4.664</td> <td>    5.326</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 6.909</td> <th>  Durbin-Watson:     </th> <td>   1.675</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.032</td> <th>  Jarque-Bera (JB):  </th> <td>   9.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.299</td> <th>  Prob(JB):          </th> <td> 0.00973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.366</td> <th>  Cond. No.          </th> <td>    6.17</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.906\n",
       "Model:                            OLS   Adj. R-squared:                  0.904\n",
       "Method:                 Least Squares   F-statistic:                     468.9\n",
       "Date:                Wed, 27 Mar 2019   Prob (F-statistic):           1.37e-50\n",
       "Time:                        23:10:16   Log-Likelihood:                -65.432\n",
       "No. Observations:                 100   AIC:                             136.9\n",
       "Df Residuals:                      97   BIC:                             144.7\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.3851      0.142      2.706      0.008       0.103       0.668\n",
       "x1             1.7702      0.193      9.150      0.000       1.386       2.154\n",
       "x2             4.9949      0.167     29.947      0.000       4.664       5.326\n",
       "==============================================================================\n",
       "Omnibus:                        6.909   Durbin-Watson:                   1.675\n",
       "Prob(Omnibus):                  0.032   Jarque-Bera (JB):                9.264\n",
       "Skew:                          -0.299   Prob(JB):                      0.00973\n",
       "Kurtosis:                       4.366   Cond. No.                         6.17\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.column_stack([np.ones(m), x1, x2])\n",
    "_, p = X.shape\n",
    "model = regression.linear_model.OLS(y, X).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal parameters\n",
    "\n",
    "Let's compare the normal equation to the output of `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Beta_0 = 0.385073881382\n",
      "Reproduced  Beta_0 = 0.385073881382\n",
      "\n",
      "statsmodels Beta_1 = 1.77016602447\n",
      "Reproduced  Beta_1 = 1.77016602447\n",
      "\n",
      "statsmodels Beta_2 = 4.99486380715\n",
      "Reproduced  Beta_2 = 4.99486380715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "for i in range(len(beta)):\n",
    "    print(\"statsmodels Beta_{0} = {1}\".format(i, model.params[i]))\n",
    "    print(\"Reproduced  Beta_{0} = {1}\".format(i, beta[i]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-squared\n",
    "\n",
    "We introduced $R^2$ [here](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/linreg.ipynb). It's defined by\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^m(y^{(i)} - f^{(i)})^2}{\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2},\n",
    "\\end{equation}\n",
    "\n",
    "where $f(x)$ is any model of the data and\n",
    "\n",
    "\\begin{equation}\n",
    "f^{(i)} = f(x^{(i)}), \\qquad\n",
    "\\bar{y} = \\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}.\n",
    "\\end{equation}\n",
    "\n",
    "In the case of multiple linear regression:\n",
    "\n",
    "\\begin{equation}\n",
    "f^{(i)} = \\hat{\\beta}^Tx^{(i)} = [X\\hat{\\beta}]_i.\n",
    "\\end{equation}\n",
    "\n",
    "If we let $\\hat{f}$ be the vector of all preditions on input data:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f} \\equiv \\begin{pmatrix}f^{(1)} \\\\ f^{(2)} \\\\ \\vdots \\\\ f^{(m)} \\end{pmatrix},\n",
    "\\end{equation}\n",
    "\n",
    "then\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f} = X\\hat{\\beta}.\n",
    "\\end{equation}\n",
    "\n",
    "Let $\\hat{e}$ denote the vector of all prediction errors (or *residuals*) on input data\n",
    "\n",
    "\\begin{equation}\n",
    "e^{(i)} = y^{(i)} - f^{(i)}, \\qquad\n",
    "\\hat{e} \\equiv \\begin{pmatrix}e^{(1)} \\\\ e^{(2)} \\\\ \\vdots \\\\ e^{(m)} \\end{pmatrix} = Y - \\hat{f} = Y - X\\hat{\\beta}.\n",
    "\\end{equation}\n",
    "\n",
    "`statsmodels` stores the residuals in the attribute `resid`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels first five residuals = [-0.29277296  0.02056819  0.16915124  0.77827397  0.2356304 ]\n",
      "Reproduced  first five residuals = [-0.29277296  0.02056819  0.16915124  0.77827397  0.2356304 ]\n"
     ]
    }
   ],
   "source": [
    "f = X.dot(beta)\n",
    "e = y - f\n",
    "print(\"statsmodels first five residuals = {}\".format(model.resid[:5]))\n",
    "print(\"Reproduced  first five residuals = {}\".format(e[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute $R^2$ from scratch and compare the result with that of `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels R-squared = 0.906264829007\n",
      "Reproduced  R-squared = 0.906264829007\n"
     ]
    }
   ],
   "source": [
    "ymean = y.mean()\n",
    "\n",
    "rsquared = 1 - np.dot(e, e) / np.dot(y - ymean, y - ymean)\n",
    "print(\"statsmodels R-squared = {}\".format(model.rsquared))\n",
    "print(\"Reproduced  R-squared = {}\".format(rsquared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like simple linear regression, $R^2$ is equal to the square of the sample correlation between $f$ and $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9062648290073423"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(f, y)[0, 1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proved this property mathematically for simple linear regression. In what follows, we will generalize the proof to multiple linear regression.\n",
    "\n",
    "Recall that minimizing the cost function gave us the following equation: $X^T(X\\hat{\\beta} - Y)=0$, which can also be expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "X^T\\hat{e} = 0.\n",
    "\\end{equation}\n",
    "\n",
    "Looking at the first component of this equation:\n",
    "\n",
    "\\begin{equation}\n",
    "[X^T\\hat{e}]_0 = \\sum_{i=1}^{m}X_{i0}e^{(i)} = \\sum_{i=1}^{m}x^{(i)}_0e^{(i)} = \\sum_{i=1}^{m}e^{(i)} = 0,\n",
    "\\end{equation}\n",
    "\n",
    "we see that the sum over residuals is zero. Since $e^{(i)}=y^{(i)} - f^{(i)}$, this implies\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{m}y^{(i)} = \\sum_{i=1}^{m} f^{(i)},\n",
    "\\end{equation}\n",
    "\n",
    "or equivalently\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{f} = \\bar{y},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{f} \\equiv \\frac{1}{m} \\sum_{i=1}^{m} f^{(i)}.\n",
    "\\end{equation}\n",
    "\n",
    "Also, if we apply $\\hat{\\beta}$ to both sides of $X^T\\hat{e} = 0$, we find:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}^TX^T\\hat{e} = (X\\hat{\\beta})^T\\hat{e} = \\hat{f}^T\\hat{e} = 0.\n",
    "\\end{equation}\n",
    "\n",
    "Alright, on to computing $R^2$. Let's start with the denominator:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{m}(y^{(i)} - \\bar{y})^2\n",
    "    &= \\sum_{i=1}^{m}[(y^{(i)} - f^{(i)}) + (f^{(i)} - \\bar{y})]^2 \\\\\n",
    "    &= \\sum_{i=1}^{m}[e^{(i)} + (f^{(i)} - \\bar{y})]^2 \\\\\n",
    "    &= \\sum_{i=1}^{m}(e^{(i)})^2 + \\sum_{i=1}^{m}(f^{(i)} - \\bar{y})^2 + 2\\sum_{i=1}^{m}(f^{(i)} - \\bar{y})e^{(i)} \\\\\n",
    "    &= \\sum_{i=1}^{m}(e^{(i)})^2 + \\sum_{i=1}^{m}(f^{(i)} - \\bar{y})^2 + 2\\hat{f}^T\\hat{e} - 2\\bar{y}\\sum_{i=1}^{m}e^{(i)} \\\\\n",
    "    &= \\sum_{i=1}^{m}(e^{(i)})^2 + \\sum_{i=1}^{m}(f^{(i)} - \\bar{y})^2 \\\\\n",
    "    &= \\sum_{i=1}^{m}(e^{(i)})^2 + \\sum_{i=1}^{m}(f^{(i)} - \\bar{f})^2\n",
    "\\end{align*}\n",
    "\n",
    "where we have used $\\sum_{i=1}^{m}e^{(i)}=0$ and $\\hat{f}^T\\hat{e}=0$ in the second last equality, and $\\bar{y}=\\bar{f}$ in the last one. Plugging this back into the definition of $R^2$, we find:\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = \\frac{\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2 - \\sum_{i=1}^m(e^{(i)})^2}{\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2}\n",
    "    = \\frac{\\sum_{i=1}^{m}(f^{(i)} - \\bar{f})^2}{\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2}.\n",
    "\\end{equation}\n",
    "\n",
    "The correlation $r_{fy}$ between $f$ and $y$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "r_{fy} = \\frac{C_{fy}}{S_fS_y},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "S_y = \\frac{1}{m}\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2, \\qquad\n",
    "S_f = \\frac{1}{m}\\sum_{i=1}^m (f^{(i)} - \\bar{f})^2, \\qquad\n",
    "C_{fy} = \\frac{1}{m}\\sum_{i=1}^m (f^{(i)} - \\bar{f})(y^{(i)} - \\bar{y}).\n",
    "\\end{equation}\n",
    "\n",
    "Given these definitions, we can also rewrite $R^2$ more simply as\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = \\frac{S_f^2}{S_y^2}.\n",
    "\\end{equation}\n",
    "\n",
    "The covariance between $f$ and $y$ can be further simplified:\n",
    "\n",
    "\\begin{align*}\n",
    "C_{fy}\n",
    "    &= \\frac{1}{m}\\sum_{i=1}^{m} (f^{(i)} - \\bar{f})(y^{(i)} - \\bar{y}) \\\\\n",
    "    &= \\frac{1}{m}\\sum_{i=1}^{m} (f^{(i)} - \\bar{f})(e^{(i)} + f^{(i)} - \\bar{f}) \\\\\n",
    "    &= \\frac{1}{m}\\sum_{i=1}^{m} (f^{(i)} - \\bar{f})^2 + \\frac{1}{m}\\sum_{i=1}^{m} f^{(i)}e^{(i)} - \\frac{\\bar{f}}{m}\\sum_{i=1}^{m}e^{(i)} \\\\\n",
    "    &= S_f^2,\n",
    "\\end{align*}\n",
    "\n",
    "where the second line uses the definition of $e^{(i)}$ and $\\bar{y}=\\bar{f}$, and the last equality follows from $\\sum_{i=1}^{m}e^{(i)}=0$ and $\\hat{f}^T\\hat{e}=0$.  Plugging this back in $r_{fy}$:\n",
    "\n",
    "\\begin{equation}\n",
    "r_{fy}^2 = \\frac{C_{fy}^2}{S_f^2S_y^2} = \\frac{S_f^4}{S_f^2S_y^2} = \\frac{S_f^2}{S_y^2} = R^2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic view\n",
    "\n",
    "The probabilistic view is much the same as before. We will assume the following model of the data:\n",
    "\n",
    "\\begin{equation}\n",
    "Y = X\\beta + \\epsilon, \\qquad\n",
    "\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m),\n",
    "\\end{equation}\n",
    "\n",
    "where $I_m$ is the $m$-dimensional identity matrix. We think of $\\epsilon$ as the error or noise due to factors we cannot explain. Here we're assuming that errors associated with different observations are uncorrelated normals with the same variance $\\sigma^2$. With this assumption, the probabiliy of observing the dataset is\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\beta, \\sigma^2)\n",
    "    &= \\frac{1}{(2\\pi \\sigma^2)^{m/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}(Y-X\\beta)^T(Y - X\\beta)\\right]} \\\\\n",
    "    &= \\frac{1}{(2\\pi \\sigma^2)^{m/2}}\\exp{\\left(-\\frac{mJ(\\beta)}{\\sigma^2}\\right)}.\n",
    "\\end{align*}\n",
    "\n",
    "As before, the parameters $\\hat{\\beta}$ that maximize $P$ are precisely those that minimize the cost function $J(\\beta)$. Also, we can reproduce `statsmodels`'s computation of log-likelihood by evaluating $\\log P(\\hat{\\beta}, \\tilde{\\sigma}^2)$, where $\\tilde{\\sigma}^2$ is the biased sample estimate of the variance of residuals:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\sigma}^2 = \\frac{1}{m}\\hat{e}^T\\hat{e}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Log-Likelihood = -65.4317634738\n",
      "Reproduced  Log-Likelihood = -65.4317634738\n"
     ]
    }
   ],
   "source": [
    "loglikelihood = -(m/2.0)*np.log(2*np.pi*np.var(e)) - m/2.0\n",
    "print(\"statsmodels Log-Likelihood = {}\".format(model.llf))\n",
    "print(\"Reproduced  Log-Likelihood = {}\".format(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error of estimates\n",
    "\n",
    "What is the probability distribution of $\\hat{\\beta}$? Since $Y$ has a multivariate Gaussian distribution and $\\hat{\\beta}$ is a linear transformation of $Y$, $\\hat{\\beta}$ itself has a multivariate Gaussian distribution. This is a general result which we won't prove here. Instead, we will compute the mean and covariance of $\\hat{\\beta}$.\n",
    "\n",
    "Before that, let's establish some notation that will make our lives easier when dealing with expectation value of matrix elements: if $V$ is a matrix of random variables, $\\mathbb{E}[V]$ will denote the matrix of the expectation value of the elements, i.e. $\\mathbb{E}[V]_{ij} \\equiv \\mathbb{E}[V_{ij}]$. How will this help us? Consider $V=AZ$, where $Z$ is a matrix of random variables and $A$ is a fixed matrix. Then, taking the expectation value of the elements of $V$ is as easy as $\\mathbb{E}[V]=A\\mathbb{E}[Z]$ because:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[V_{ij}]\n",
    "    &= \\mathbb{E}\\left[\\sum_{k}A_{ik}Z_{kj}\\right] \\\\\n",
    "    &= \\sum_{k}A_{ik}\\mathbb{E}[Z_{kj}] \\\\\n",
    "    &= \\sum_{k}A_{ik}\\mathbb{E}[Z]_{kj} \\\\\n",
    "    &= [A\\mathbb{E}[Z]]_{ij}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Alright, we can now compute the expectation value of $\\hat{\\beta}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[\\hat{\\beta}]\n",
    "    &= (X^TX)^{-1}X^T\\mathbb{E}[Y] \\\\\n",
    "    &= (X^TX)^{-1}X^T(X\\beta + \\mathbb{E}[\\epsilon]) \\\\\n",
    "    &= (X^TX)^{-1}X^TX\\beta \\\\\n",
    "    &=\\beta.\n",
    "\\end{align*}\n",
    "\n",
    "Good! Our point estimates are unbiased. Let's move on the covariance matrix. Note that\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\beta} - \\beta\n",
    "    &= (X^TX)^{-1}X^TY - \\beta \\\\\n",
    "    &= (X^TX)^{-1}X^TY - (X^TX)^{-1}X^TX\\beta \\\\\n",
    "    &= (X^TX)^{-1}X^T(Y - X\\beta) \\\\\n",
    "    &= (X^TX)^{-1}X^T\\epsilon.\n",
    "\\end{align*}\n",
    "\n",
    "It then follows that\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[(\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)^T]\n",
    "    &= (X^TX)^{-1}X^T \\mathbb{E}[\\epsilon \\epsilon^T] X (X^TX)^{-1} \\\\\n",
    "    &= (X^TX)^{-1}X^T (\\sigma^2 I_m)  X (X^TX)^{-1} \\\\\n",
    "    &= \\sigma^2 (X^TX)^{-1},\n",
    "\\end{align*}\n",
    "\n",
    "where we have used $\\mathbb{E}[\\epsilon \\epsilon^T] = \\sigma^2 I_m$. What an elegant expression! For simple linear regression we had to do a lot more work to compute the the variance of $\\hat{\\alpha}$, $\\hat{\\beta}$, and also their covariance. Above we computed $(X^TX)^{-1}$ for univariate linear regression. Using that, it's easy to check that the formulas derived [here](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/linreg.ipynb) are consistent with the covariance matrix $\\sigma^2(X^TX)^{-1}$.\n",
    "\n",
    "To summarize, $\\hat{\\beta}$ has a multivariate Gaussian distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} \\sim \\mathcal{N}(\\beta, \\sigma^2(X^TX)^{-1}).\n",
    "\\end{equation}\n",
    "\n",
    "Let's check this against `statsmodels`: (we will use `statsmodels` estimate of $\\sigma^2$ by using the `scale` attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Beta_0-Beta_0 covariance = 0.0202562993414\n",
      "Reproduced  Beta_0-Beta_0 covariance = 0.0202562993414\n",
      "\n",
      "statsmodels Beta_0-Beta_1 covariance = -0.0202764718329\n",
      "Reproduced  Beta_0-Beta_1 covariance = -0.0202764718329\n",
      "\n",
      "statsmodels Beta_0-Beta_2 covariance = -0.0155511241659\n",
      "Reproduced  Beta_0-Beta_2 covariance = -0.0155511241659\n",
      "\n",
      "statsmodels Beta_1-Beta_1 covariance = 0.0374243151515\n",
      "Reproduced  Beta_1-Beta_1 covariance = 0.0374243151515\n",
      "\n",
      "statsmodels Beta_1-Beta_2 covariance = 0.00299109336668\n",
      "Reproduced  Beta_1-Beta_2 covariance = 0.00299109336668\n",
      "\n",
      "statsmodels Beta_2-Beta_2 covariance = 0.0278181525471\n",
      "Reproduced  Beta_2-Beta_2 covariance = 0.0278181525471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beta_cov = model.scale * np.linalg.inv(X.T.dot(X))\n",
    "statsmodels_cov = model.cov_params()\n",
    "\n",
    "for i in range(len(beta)):\n",
    "    for j in range(i, len(beta)):\n",
    "        print(\"statsmodels Beta_{0}-Beta_{1} covariance = {2}\".format(i, j, statsmodels_cov[i, j]))\n",
    "        print(\"Reproduced  Beta_{0}-Beta_{1} covariance = {2}\".format(i, j, beta_cov[i, j]))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the unbiased estimate of $\\sigma^2$? Note that the residuals can be written as\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{e}\n",
    "    &= Y - X\\hat{\\beta} \\\\\n",
    "    &= Y - X(X^TX)^{-1}X^TY \\\\\n",
    "    &= (I_m - X(X^TX)^{-1}X^T)Y \\\\\n",
    "    &= MY,\n",
    "\\end{align*}\n",
    "\n",
    "where we've introduced the matrix $M$:\n",
    "\n",
    "\\begin{equation}\n",
    "M \\equiv I - X(X^TX)^{-1}X^T.\n",
    "\\end{equation}\n",
    "\n",
    "It's easy to check that $M$ is idempotent, i.e. it's symmetric and also equal to its own square: $M^2=M$. Also, it's easy to check that $MX=0$. The covariance of $\\hat{e}$ is given by\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[\\hat{e}\\hat{e}^T]\n",
    "    &= M\\mathbb{E}[Y Y^T] M^T \\\\\n",
    "    &= M \\mathbb{E}[(X\\beta + \\epsilon)(X\\beta + \\epsilon)^T] M^T \\\\\n",
    "    &= M \\left[X\\beta(X\\beta)^T + X\\beta \\mathbb{E}[\\epsilon]^T + \\mathbb{E}[\\epsilon] (X \\beta)^T + \\mathbb{E}[\\epsilon \\epsilon^T]\\right] M^T \\\\\n",
    "    &= (MX)\\beta\\beta^T(MX)^T + \\sigma^2MM^T \\\\\n",
    "    &= \\sigma^2MM^T \\\\\n",
    "    &= \\sigma^2M.\n",
    "\\end{align*}\n",
    "\n",
    "The expectation value of the sum of squared residuals is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}[\\hat{e}^T\\hat{e}]\n",
    "    = \\text{tr}(\\mathbb{E}[\\hat{e}\\hat{e}^T]) \\\\\n",
    "    = \\sigma^2 \\text{tr}(M).\n",
    "\\end{equation}\n",
    "\n",
    "Next, we need to find the trace of $M$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{tr}(M)\n",
    "    &= \\text{tr}(I_m) - \\text{tr}(X(X^TX)^{-1}X^T) \\\\\n",
    "    &= m - \\text{tr}(X^TX(X^TX)^{-1}) \\\\\n",
    "    &= m - (n+1),\n",
    "\\end{align*}\n",
    "\n",
    "where the second equality uses the linear algebra identity $\\text{tr}(AB)=\\text{tr}(BA)$. Therefore\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}[\\hat{e}^T\\hat{e}] = (m - n - 1)\\sigma^2,\n",
    "\\end{equation}\n",
    "\n",
    "from which it follows that the following estimator of $\\sigma^2$ is unbiased:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\sigma}^2 = \\frac{1}{m - n - 1}\\hat{e}^T\\hat{e}.\n",
    "\\end{equation}\n",
    "\n",
    "For simple linear regression, where $n=1$, we recover the $m-2$ factor in the denominator. Let's see if this reproduces the estimate of `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels estimate of sigma^2 = 0.223401966576\n",
      "Reproduced  estimate of sigma^2 = 0.223401966576\n"
     ]
    }
   ],
   "source": [
    "yvar = np.dot(e, e)/(m - len(beta))\n",
    "print(\"statsmodels estimate of sigma^2 = {}\".format(model.scale))\n",
    "print(\"Reproduced  estimate of sigma^2 = {}\".format(yvar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence intervals\n",
    "\n",
    "Let's work out confidence intervals for $\\hat{\\beta}_j$, so we can make statements like: there's a $95\\%$ chance that the interval $(\\beta^{(L)}_j, \\beta^{(U)}_j)$ contains $\\beta_j$. We've already shown that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}_j \\sim \\mathcal{N}(\\beta_j, \\sigma^2\\Sigma_{jj}),\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "\\Sigma \\equiv (X^TX)^{-1}.\n",
    "\\end{equation}\n",
    "\n",
    "This means $\\beta^{(L)}_j=\\hat{\\beta}_j-z\\sqrt{\\sigma^2\\Sigma_{jj}}$ and $\\beta^{(U)}_j=\\hat{\\beta}_j+z\\sqrt{\\sigma^2\\Sigma_{jj}}$ define a valid confidence interval for confidence level $1 - \\gamma$, where $z=\\Phi^{-1}(1-\\gamma/2)$ and $\\Phi_t^{-1}$ is the inverse CDF of the standard normal distribution:\n",
    "\n",
    "\\begin{align*}\n",
    "P\\left(\\hat{\\beta}_j-z \\sqrt{\\sigma^2\\Sigma_{jj}} \\le \\beta_j \\le \\hat{\\beta}_j+z\\sqrt{\\sigma^2\\Sigma_{jj}}\\right)\n",
    "    &= P\\left(-z \\le \\frac{\\hat{\\beta}_j - \\beta_j}{\\sqrt{\\sigma^2\\Sigma_{jj}}} \\le z \\right) \\\\\n",
    "    &= 2\\Phi(z)-1 \\\\\n",
    "    &= 2\\Phi(\\Phi^{-1}(1-\\gamma/2))-1 \\\\\n",
    "    &= 1-\\gamma.\n",
    "\\end{align*}\n",
    "\n",
    "In other words, there's a $1-\\gamma$ chance that the interval $(\\beta^{(L)}_j, \\beta^{(U)}_j)$ contains $\\beta_j$. The problem, though, is that we do not know $\\sigma^2$. What if we replace it with the estimate $\\hat{\\sigma}^2$? Then we'll need to know the distribution of\n",
    "\n",
    "\\begin{align*}\n",
    "T\n",
    "    &\\equiv \\frac{\\hat{\\beta}_j - \\beta_j}{\\sqrt{\\hat{\\sigma}^2\\Sigma_{jj}}} \\\\\n",
    "    &= \\frac{(\\hat{\\beta}_j - \\beta_j)/\\sqrt{\\sigma^2\\Sigma_{jj}}}{\\sqrt{\\frac{1}{(m - n - 1)\\sigma^2}\\hat{e}^T\\hat{e}}} \\\\\n",
    "    &= \\frac{Z}{\\sqrt{\\frac{V}{m-n-1}}},\n",
    "\\end{align*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "Z \\equiv \\frac{\\hat{\\beta}_j - \\beta_j}{\\sqrt{\\sigma^2\\Sigma_{jj}}}, \\qquad\n",
    "V \\equiv \\frac{1}{\\sigma^2}\\hat{e}^T\\hat{e}.\n",
    "\\end{equation}\n",
    "\n",
    "We already know that $Z \\sim \\mathcal{N}(0, 1)$. It can also be shown that $V$ has a chi-squared distribution with $m-n-1$ degrees of freedom and that $Z$ and $V$ are independent. Therefore, $T$ has a t-distribution with $m-n-1$ degrees of freedom. (This is not trivial to show. We presented a proof for simple linear regression using Cochran's theorem. For some reason I haven't bothered working it out for multiple linear regression. I suppose every now and then I choose not to obsess.)\n",
    "\n",
    "This means that $\\beta^{(L)}_j=\\hat{\\beta}_j-t\\sqrt{\\hat{\\sigma}^2\\Sigma_{jj}}$ and $\\beta^{(U)}_j=\\hat{\\beta}_j+t\\sqrt{\\hat{\\sigma}^2\\Sigma_{jj}}$ define a valid confidence interval for confidence level $1-\\gamma$ ($0\\le \\gamma \\le 1$), where $t=\\Phi_t^{-1}(1-\\gamma/2)$ and $\\Phi_t^{-1}$ is the inverse CDF of the t-distribution with $m-n-1$ degrees of freedom. This gives us the same confidence interval as `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Beta_0 95% confidence interval = (0.102598895008, 0.667548867756)\n",
      "Reproduced  Beta_0 95% confidence interval = (0.102598895008, 0.667548867756)\n",
      "\n",
      "statsmodels Beta_1 95% confidence interval = (1.38621407759, 2.15411797136)\n",
      "Reproduced  Beta_1 95% confidence interval = (1.38621407759, 2.15411797136)\n",
      "\n",
      "statsmodels Beta_2 95% confidence interval = (4.66383629856, 5.32589131574)\n",
      "Reproduced  Beta_2 95% confidence interval = (4.66383629856, 5.32589131574)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# statsmodel confidence interval\n",
    "statsmodels_ci = model.conf_int()\n",
    "t = stats.t.ppf(1 - 0.05/2., m - len(beta))\n",
    "\n",
    "for i in range(len(beta)):\n",
    "    beta_se = np.sqrt(beta_cov[i, i])\n",
    "    print(\"statsmodels Beta_{0} 95% confidence interval = ({1}, {2})\".format(i, statsmodels_ci[i, 0], statsmodels_ci[i, 1]))\n",
    "    print(\"Reproduced  Beta_{0} 95% confidence interval = ({1}, {2})\".format(i, beta[i] - t*beta_se, beta[i] + t*beta_se))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For unseen values of the independent variables $x$, we predict $y$ using $\\hat{f}(x) \\equiv \\hat{\\beta}^Tx$, which is subject to statistical noise. The mean is what we want:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}[\\hat{f}(x)] = \\mathbb{E}[\\hat{\\beta}]^T x = \\beta^Tx.\n",
    "\\end{equation}\n",
    "\n",
    "The variance is then given by\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Var}[\\hat{f}]\n",
    "    &= \\mathbb{E}\\left[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])^2 \\right]\n",
    "    &= x^T\\mathbb{E}[(\\hat{\\beta} - \\beta) (\\hat{\\beta} - \\beta)^T] x \\\\\n",
    "    &= \\sigma^2 x^T (X^TX)^{-1}x.\n",
    "\\end{align*}\n",
    "\n",
    "Does this match the expression we derived for simple linear regression? Let's check:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma^2 x^T (X^TX)^{-1}x\n",
    "    &= \\frac{\\sigma^2}{m S_x^2}\n",
    "    \\begin{pmatrix} 1 \\\\ x \\end{pmatrix}^T\n",
    "    \\begin{pmatrix} S_x^2 + \\bar{x}^2 && -\\bar{x} \\\\ -\\bar{x} && 1 \\end{pmatrix}\n",
    "    \\begin{pmatrix} 1 \\\\ x \\end{pmatrix} \\\\\n",
    "    &= \\frac{\\sigma^2}{m S_x^2} (S_x^2 + \\bar{x}^2 - 2x\\bar{x} +x^2) \\\\\n",
    "    &= \\frac{\\sigma^2}{m S_x^2} (S_x^2 + (x - \\bar{x})^2) \\\\\n",
    "    &= \\frac{\\sigma^2}{m} \\left[1 + \\frac{(x - \\bar{x})^2}{S_x^2}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "Indeed! To get confidence intervals for our predictions, we will proceed as before: $f_L=\\hat{f}-ts_{\\hat{f}}$ and $f_U=\\hat{f}+ts_{\\hat{f}}$ define a valid confidence interval for confidence level $1-\\gamma$ ($0\\le \\gamma \\le 1$), where $t=\\Phi_t^{-1}(1-\\gamma/2)$, $\\Phi_t^{-1}$ is the inverse CDF of the t-distribution with $m-n-1$ degrees of freedom, and\n",
    "\n",
    "\\begin{equation}\n",
    "s_{\\hat{f}}^2 = \\hat{\\sigma}^2 x^T (X^TX)^{-1}x.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-test\n",
    "\n",
    "Suppose we want to test a null hypothesis involving linear combinations of $\\beta_{0}, \\beta_{1}, \\dots, \\beta_{n}$. For instance, we may be interested in testing the null hypothesis: $\\beta_0 = 1$ and $\\beta_0+\\beta_1=0$. We can summarize $q$ independent linear constraints on $\\beta$ using the matrix equation:\n",
    "\n",
    "\\begin{equation}\n",
    "R\\beta = r,\n",
    "\\end{equation}\n",
    "\n",
    "where $R$ is a $q \\times (n+1)$ matrix and $r$ is a $q$-dimensional vector. We will assume that all constraints are independent, i.e. $\\text{rank}(R)=q$.\n",
    "\n",
    "Let's a take a small detour into the world of linear algebra. Suppose $A$ is a symmetric positive definite matrix, i.e. $v^TAv > 0$ for all vectors $v \\neq 0$. Then, it admits the following diagonalization:\n",
    "\n",
    "\\begin{equation}\n",
    "A=UDU^T, \\qquad\n",
    "U^TU=UU^T=I, \\qquad\n",
    "D_{ij}=\\lambda_i\\delta_{ij}, \\qquad\n",
    "\\lambda_i > 0.\n",
    "\\end{equation}\n",
    "\n",
    "For any real number $\\alpha$, we can define $A^{\\alpha}$ as\n",
    "\n",
    "\\begin{equation}\n",
    "A^{\\alpha}\\equiv UD^{\\alpha}U^T, \\qquad\n",
    "[D^{\\alpha}]_{ij}=\\lambda_i^{\\alpha}\\delta_{ij}.\n",
    "\\end{equation}\n",
    "\n",
    "For instance, to compute $A^{-1/2}$, we raise all diagonal elements of $D$ to the power of $-1/2$ (which we can do because they are all strictly positive), and then sandwich the result between $U$ and $U^T$. This definition has several nice properties. For instance:\n",
    "\n",
    "\\begin{align*}\n",
    "A^{\\alpha_1}A^{\\alpha_2}\n",
    "    &= UD^{\\alpha_1}(U^TU)D^{\\alpha_2}U^T \\\\\n",
    "    &= UD^{\\alpha_1}D^{\\alpha_2}U^T \\\\\n",
    "    &= UD^{\\alpha_1+\\alpha_2}U^T \\\\\n",
    "    &= A^{\\alpha_1+\\alpha_2},\n",
    "\\end{align*}\n",
    "\n",
    "where $D^{\\alpha_1}D^{\\alpha_2}=D^{\\alpha_1+\\alpha_2}$ follows from the fact that $D$ is diagonal. Moreover, $A^{\\alpha}$ is also positive definite:\n",
    "\n",
    "\\begin{align*}\n",
    "v^T A^{\\alpha} v\n",
    "    &= v^T U D^{\\alpha} U^T v \\\\\n",
    "    &= (U^Tv)D^{\\alpha}(U^Tv) \\\\\n",
    "    &= \\sum_{ij}[D^{\\alpha}]_{ij}[U^Tv]_{i}[U^Tv]_{j} \\\\\n",
    "    &= \\sum_{ij}(\\lambda_{i}^2)^{\\alpha}\\delta_{ij}[U^Tv]_{i}[U^Tv]_{j} \\\\\n",
    "    &= \\sum_{i}(\\lambda_{i}^2)^{\\alpha}[U^Tv]_{i}^2 \\\\\n",
    "    &> 0.\n",
    "\\end{align*}\n",
    "\n",
    "Note that the strict inequality holds because the norm of $U^Tv$ is the same as $v$:\n",
    "\n",
    "\\begin{equation}\n",
    "[U^Tv]^T[U^Tv] = v^T(UU^T)v=v^Tv,\n",
    "\\end{equation}\n",
    "\n",
    "so not all of its elements can be zero because by assumption $v \\neq 0$.\n",
    "\n",
    "Alright, back to business. Consider the following matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "B \\equiv R (X^TX)^{-1} R^T.\n",
    "\\end{equation}\n",
    "\n",
    "Let's show it's positive definite:\n",
    "\n",
    "\\begin{align*}\n",
    "v^T B v\n",
    "    &= v^T R (X^TX)^{-1} R^T v \\\\\n",
    "    &= (R^Tv)^T(X^TX)^{-1} (R^Tv) \\\\\n",
    "    &> 0.\n",
    "\\end{align*}\n",
    "\n",
    "There's quite a lot going on in the last inequality, so let's unpack it. First, without any restrictions on $X$, $X^TX$ is positive *semi*-definite: $v^T(X^TX)v=(Xv)^T(Xv)\\ge 0$. (Note we're using $\\ge$ instead of $>$ since $Xv$ can generically be zero for some $v \\neq 0$.) Since we're assuming $X^TX$ is invertible, though, it cannot have any zero eigenvalues, so it's actually positive definite. Above we showed that if $A$ is positive definite, so is $A^{\\alpha}$. It then follows that the inverse of $X^TX$ is also positive definite, since the inverse of $X^TX$ is equal to $X^TX$ raised to the power of $-1$. Therefore, $(R^Tv)^T(X^TX)^{-1} (R^Tv)$ is strictly positive for all non-zero $R^Tv$. What we really want to show, though, is that this holds for all non-zero $v$ (as opposed to $R^Tv$). Is $R^Tv$ only zero when $v=0$? Yes, because we're assuming all rows of $R$ are linearly independent (i.e. $\\text{rank}(R)=q$), which is equivalent to saying $R^Tv=0$ implies $v=0$.\n",
    "\n",
    "\n",
    "Having shown $B$ is positive definite, we can raise it to the power of $-1/2$ and introduce the following quantity:\n",
    "\n",
    "\\begin{equation}\n",
    "Z \\equiv \\frac{1}{\\sigma} B^{-1/2}R(\\hat{\\beta} - \\beta).\n",
    "\\end{equation}\n",
    "\n",
    "Since $Z$ is a linear transformation of $\\hat{\\beta}$, it has a multivariate Gaussian distribution. It's clear that it has zero mean, since $\\mathbb{E}[\\hat{\\beta}] = \\beta$. Let's compute its covariance matrix:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[ZZ^T]\n",
    "    &= \\frac{1}{\\sigma^2} B^{-1/2}R \\mathbb{E}[(\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)^T] R^T B^{-1/2} \\\\\n",
    "    &= \\frac{1}{\\sigma^2} B^{-1/2}R \\left[\\sigma^2 (X^TX)^{-1} \\right] R^T B^{-1/2} \\\\\n",
    "    &= B^{-1/2}\\left[R (X^TX)^{-1}R^T \\right] B^{-1/2} \\\\\n",
    "    &= B^{-1/2}BB^{-1/2} \\\\\n",
    "    &= I_q.\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, $Z$ is a $q$-dimensional vector of independent standard normals, which in turn implies that $Z^TZ \\sim \\chi^2_{q}$. We had previously shown that $\\hat{e}^T\\hat{e}/\\sigma^2 \\sim \\chi^2_{m-n-1}$. The following ratio of these two quantities\n",
    "\n",
    "\\begin{equation}\n",
    "f = \\frac{(Z^TZ)/q}{(\\hat{e}^T\\hat{e}/\\sigma^2)/(m - n - 1)}.\n",
    "\\end{equation}\n",
    "\n",
    "has the [F(q, m-n-1)](https://en.wikipedia.org/wiki/F-distribution) distribution. (We also have to show that $Z^TZ$ and $\\hat{e}^T\\hat{e}$ are independent. This is true, but we won't prove it.) Let's compute $Z^TZ$:\n",
    "\n",
    "\\begin{align*}\n",
    "Z^TZ\n",
    "    &= \\frac{1}{\\sigma^2} (R\\hat{\\beta} - R\\beta)^TB^{-1/2}B^{-1/2}(R\\hat{\\beta} - R\\beta) \\\\\n",
    "    &= \\frac{1}{\\sigma^2} (R\\hat{\\beta} - r)^TB^{-1}(R\\hat{\\beta} - r) \\\\\n",
    "    &= \\frac{1}{\\sigma^2} (R\\hat{\\beta} - r)^T\\left[R (X^TX)^{-1}R^T \\right]^{-1}(R\\hat{\\beta} - r),\n",
    "\\end{align*}\n",
    "\n",
    "where we have used the null hypothesis $R\\beta = r$. Plugging this back in $f$ we find:\n",
    "\n",
    "\\begin{equation}\n",
    "f = \\frac{(R\\hat{\\beta} - r)^T\\left[R (X^TX)^{-1}R^T \\right]^{-1}(R\\hat{\\beta} - r)/q}{\\hat{e}^T\\hat{e}/(m - n - 1)}.\n",
    "\\end{equation}\n",
    "\n",
    "Note that $\\sigma^2$ drops out of the formula, so $f$ can be computed from the sample.\n",
    "\n",
    "\n",
    "The F-statistic, and its corresponding p-value, reported by `statsmodels` is based on the following null hypothesis: $\\beta_1=\\beta_2=\\dots=\\beta_n=0$. In other words, it assumes that $y$ does not depend on any of the explanatory variables $x_1, \\dots, x_n$ and the correct model of the data is simply $y=const$. If we cannot reject this null hypothesis at a reasonable confidence level, we should probably be concerned. In such a scenario, we will likely also see a low $R^2$, since $y=const$ is able to explain the data better than our model. The corresponding values of $r$ and $R$ for this test are:\n",
    "\n",
    "\\begin{equation}\n",
    "R = [0_{n \\times 1} I_{n \\times n}], \\qquad\n",
    "r = 0_{n \\times 1},\n",
    "\\end{equation}\n",
    "\n",
    "where $0_{n \\times 1}$ is the $n$-dimensional vector of zeros and $I_{n \\times n}$ is the $n$-dimensional identity matrix. We're now able to reproduce the F-statistic reported by `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels F-statistic = 468.915176037\n",
      "Reproduced  F-statistic = 468.915176037\n"
     ]
    }
   ],
   "source": [
    "R = np.column_stack((np.zeros(p - 1), np.identity(p - 1)))\n",
    "q, _ = R.shape\n",
    "r = np.zeros(p - 1)\n",
    "B = R.dot(np.linalg.inv(X.T.dot(X))).dot(R.T)\n",
    "\n",
    "ftest = ((R.dot(beta) - r).T.dot(np.linalg.inv(B)).dot(R.dot(beta) - r)/q) / (np.dot(e, e)/(m - p))\n",
    "print(\"statsmodels F-statistic = {}\".format(model.fvalue))\n",
    "print(\"Reproduced  F-statistic = {}\".format(ftest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting result is that for the above choice of $R$ and $r$, $f$ is also given by (see [here](https://stats.stackexchange.com/questions/258461/proof-that-f-statistic-follows-f-distribution) for proof):\n",
    "\n",
    "\\begin{align*}\n",
    "f\n",
    "    &= \\frac{m - n - 1}{n}\\frac{\\sum_{i=1}^{m}(y^{(i)} - \\bar{y})^2 - \\hat{e}^T\\hat{e}}{\\hat{e}^T\\hat{e}} \\\\\n",
    "    &= \\frac{m - n - 1}{n}\\frac{R^2}{1 - R^2}.\n",
    "\\end{align*}\n",
    "\n",
    "Let's verify this for our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468.91517603674214"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsquared / (1 - rsquared) * (m - p) / (p - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms our intuition that a low value of $f$ should correspond to a low $R^2$, since in that case the null hypothesis $y=const$ cannot be rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When the normal equation fails\n",
    "\n",
    "Can $X^TX$ ever be non-invertible? Yes! Below we will cover two such scenarios.\n",
    "\n",
    "If one of the features is a linear combination of the other ones, $X^TX$ will not be invertible. For example, suppose we include two features in our data set, but they are related via $x_2 = 2x_1 + 3$. Then the determinant of $X^TX$ will be zero, which means it's not invertible: (note that because of numerical noise, the determinant will not be exactly zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1872293550114626e-10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = np.column_stack([np.ones(m), x1, 2*x1+3])\n",
    "np.linalg.det(X2.T.dot(X2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does `statsmodels` then fail to produce the estimates $\\hat{\\beta}$? Let's check by generating data using\n",
    "\n",
    "\\begin{equation}\n",
    "y(x_1, x_2) \\sim 0.3 + x_1 + x_2 + \\epsilon, \\qquad\n",
    "\\epsilon \\sim \\mathcal{N}(0, 0.5^2).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.28363415,  0.98799979,  1.12509713])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = np.random.normal(loc=X2.dot(np.array([0.3, 1, 1])), scale=0.5, size=m)\n",
    "model2 = regression.linear_model.OLS(y2, X2).fit()\n",
    "model2.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we're still getting numbers from `statsmodels`. What's going on? Instead of using the actual inverse of $X^TX$, `statsmodels` is using the *pseudo*-inverse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.28363415,  0.98799979,  1.12509713])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X2.T.dot(X2)).dot(X2.T).dot(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly explain how the pseudo-inverse can be calculated. Consider the diagonalization of $A$: $A=U D U^T$ where $U$'s columns are orthonormal eigenvectors of $A$, i.e. $U^TU=U^TU=I$, and $D$ is a diagonal matrix whose diagonal elements are the eigenvalues of $A$. The inverse of $A$ is given by: $A^{-1}=UD^{-1}U^T$. Taking the inverse of $D$ just involves taking the numerical inverse of its diagonal elements. If one of these is zero, though, $A$ cannot be inverted. The pseudo-inverse only inverts the non-zero diagonal elements of $D$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo-inverse using numpy.linalg.pinv\n",
      "--------------------------------------\n",
      "[[ 0.06259946 -0.09530321 -0.00280806]\n",
      " [-0.09530321  0.14522771  0.00454578]\n",
      " [-0.00280806  0.00454578  0.0006674 ]]\n",
      "\n",
      "Reproduced pseudo-inverse\n",
      "--------------------------------------\n",
      "[[ 0.06259946 -0.09530321 -0.00280806]\n",
      " [-0.09530321  0.14522771  0.00454578]\n",
      " [-0.00280806  0.00454578  0.0006674 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pseudo-inverse using numpy.linalg.pinv\")\n",
    "print(\"--------------------------------------\")\n",
    "print(np.linalg.pinv(X2.T.dot(X2)))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reproduced pseudo-inverse\")\n",
    "print(\"--------------------------------------\")\n",
    "eigvals, eigvecs = np.linalg.eig(X2.T.dot(X2))\n",
    "dinv = np.diag([1/eigval if abs(eigval) > 1e-7 else 0 for eigval in eigvals])\n",
    "print(eigvecs.dot(dinv).dot(eigvecs.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $x_2$ and $x_1$ are related, we can rewrite the model as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_1, x_2)\n",
    "    = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2\n",
    "    = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2(2x_1 + 3)\n",
    "    = (\\hat{\\beta}_0 + 3\\hat{\\beta}_2) + (\\hat{\\beta}_1 + 2\\hat{\\beta}_2)x_1.\n",
    "\\end{equation}\n",
    "\n",
    "This suggests that the following new estimates should give us the exact same prediction:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\beta}_0 = \\hat{\\beta}_0 + 3\\hat{\\beta}_2, \\qquad\n",
    "\\tilde{\\beta}_1 = \\hat{\\beta}_1 + 2\\hat{\\beta}_2, \\qquad\n",
    "\\tilde{\\beta}_2 = 0.\n",
    "\\end{equation}\n",
    "\n",
    "Let's check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction with beta = [-0.28363415  0.98799979  1.12509713]: 4.38693484364\n",
      "Prediction with beta = [3.09165723 3.23819404 0.        ]: 4.38693484364\n"
     ]
    }
   ],
   "source": [
    "beta_tilde = np.array([\n",
    "    model2.params[0] + 3*model2.params[2],\n",
    "    model2.params[1] + 2*model2.params[2],\n",
    "    0\n",
    "])\n",
    "\n",
    "x_new = np.array([1, 0.4, 2*0.4 + 3])\n",
    "print(\"Prediction with beta = {0}: {1}\".format(model2.params, model2.params.dot(x_new)))\n",
    "print(\"Prediction with beta = {0}: {1}\".format(beta_tilde, beta_tilde.dot(x_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, there are infinitely many $\\hat{\\beta}$s that will produce the exact same prediction. This can be quite dangerous!\n",
    "\n",
    "Another way $X^TX$ can become non-invertible is if the number of observations is less than the number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.1390979942215958e-17"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3 = np.column_stack([np.ones(2), x1[:2], x2[:2]])\n",
    "np.linalg.det(X3.T.dot(X3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
